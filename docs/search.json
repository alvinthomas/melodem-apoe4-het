[
  {
    "objectID": "slides/01-introduction.html#hello-my-name-is-byron",
    "href": "slides/01-introduction.html#hello-my-name-is-byron",
    "title": "MELODEM data workshop",
    "section": "Hello! My name is Byron",
    "text": "Hello! My name is Byron\n\nI am an R enthusiast.\nI love dogs.\nI study risk  prediction + machine learning"
  },
  {
    "objectID": "slides/01-introduction.html#options",
    "href": "slides/01-introduction.html#options",
    "title": "MELODEM data workshop",
    "section": "Options",
    "text": "Options\nThis workshop heavily leverages R, but you may prefer not to.\n\nYou can partner up with an R user\nYou can use the simulated data (makes exercises much easier)\nYou can relax and get ☕ during exercises\n\nLearning under stress may not be ideal. Do what works best for you.\nI will do my part to make the concepts taught here R-agnostic so that you can learn plenty of valuable things no matter what language you use."
  },
  {
    "objectID": "slides/01-introduction.html#schedule",
    "href": "slides/01-introduction.html#schedule",
    "title": "MELODEM data workshop",
    "section": "Schedule",
    "text": "Schedule\nSession 1: Friday, 4:00pm - 5:30pm\n\nIntroduction, data management\n\nSession 2a: Saturday 9:30am - 11:00am\n\nDecision trees and random forests\nBreak from 10:45am - 11:00am"
  },
  {
    "objectID": "slides/01-introduction.html#schedule-1",
    "href": "slides/01-introduction.html#schedule-1",
    "title": "MELODEM data workshop",
    "section": "Schedule",
    "text": "Schedule\nSession 2b: Saturday 11:00am - 12:30am\n\nOblique random forests\nLunch from 12:30pm - 2:00pm\n\nSession 3: Saturday 2:00pm - 5:30pm\n\nCausal random forests\nBreak from 3:45pm - 4:00pm\nFinish slides or get a head start on collaboration"
  },
  {
    "objectID": "slides/01-introduction.html#schedule-2",
    "href": "slides/01-introduction.html#schedule-2",
    "title": "MELODEM data workshop",
    "section": "Schedule",
    "text": "Schedule\nSession 4: Sunday 9:30am - 12:30 pm\n\nDiscuss manuscript aims, GitHub issues (30m)\nWork in small groups (45m)\nBreak from 10:45am - 11:00am\n\nSession 5: Sunday 2:00pm - 4:00 pm\n\nProgress updates and discussion (30m)\nWork in small groups (90m)"
  },
  {
    "objectID": "slides/01-introduction.html#sticky-notes",
    "href": "slides/01-introduction.html#sticky-notes",
    "title": "MELODEM data workshop",
    "section": "Sticky notes",
    "text": "Sticky notes\nWhile you’re working on exercises,\n\nPlace pink sticky note on the back of your laptop if you want help.\nPlace blue sticky note on the back of your laptop when you are done"
  },
  {
    "objectID": "slides/01-introduction.html#goals",
    "href": "slides/01-introduction.html#goals",
    "title": "MELODEM data workshop",
    "section": "Goals",
    "text": "Goals\nGet our data organized.\n\nBuild familiarity with R/Rstudio and git/GitHub\nLearn how to use a stellar R package: targets\nPlug your data into the workshop pipeline\nHelp a friend"
  },
  {
    "objectID": "slides/01-introduction.html#whole-game",
    "href": "slides/01-introduction.html#whole-game",
    "title": "MELODEM data workshop",
    "section": "Whole game",
    "text": "Whole game\nFirst, you pull code down from the GitHub repo."
  },
  {
    "objectID": "slides/01-introduction.html#whole-game-1",
    "href": "slides/01-introduction.html#whole-game-1",
    "title": "MELODEM data workshop",
    "section": "Whole game",
    "text": "Whole game\nNext, you commit code and summary results. No data!"
  },
  {
    "objectID": "slides/01-introduction.html#whole-game-2",
    "href": "slides/01-introduction.html#whole-game-2",
    "title": "MELODEM data workshop",
    "section": "Whole game",
    "text": "Whole game\nLast, you push your code and summary results to the GitHub repo."
  },
  {
    "objectID": "slides/01-introduction.html#why-github",
    "href": "slides/01-introduction.html#why-github",
    "title": "MELODEM data workshop",
    "section": "Why GitHub?",
    "text": "Why GitHub?\nSo we can work together, separately!\n\nStore and coordinate code from multiple authors\nPublic facing team science\nFree website for our work (i.e., this workshop)."
  },
  {
    "objectID": "slides/01-introduction.html#set-up-r-packages",
    "href": "slides/01-introduction.html#set-up-r-packages",
    "title": "MELODEM data workshop",
    "section": "Set-up R packages",
    "text": "Set-up R packages\nMake sure we all have up-to-date versions of these R packages:\n\n# Install required packages for the workshop\npkgs &lt;- \n  c(\"tidyverse\", \"tidymodels\", \"data.table\", \"haven\", \"magrittr\",\n    \"glue\", \"grf\", \"aorsf\", \"glmnet\", \"xgboost\", \"randomForestSRC\",\n    \"party\", \"riskRegression\", \"survival\", \"officer\", \"flextable\", \n    \"table.glue\", \"gtsummary\", \"usethis\", \"cli\", \"ggforce\",\n    \"rpart\", \"rpart.plot\", \"ranger\", \"withr\", \"gt\", \"recipes\", \n    \"butcher\", \"sandwich\", \"lmtest\", \"gbm\", \"officedown\", \"Matrix\",\n    \"ggsurvfit\", \"tidycmprsk\", \"here\", \"tarchetypes\", \"targets\")\n\ninstall.packages(pkgs)"
  },
  {
    "objectID": "slides/01-introduction.html#pull",
    "href": "slides/01-introduction.html#pull",
    "title": "MELODEM data workshop",
    "section": "Pull!",
    "text": "Pull!\nMake sure you have a GitHub account with personal access token (PAT) stored in Rstudio\n\n\n\nOpen Rstudio\nCopy/paste the code on this slide into an R script\nImportant: adjust destdir\nRun\n\n\n\nlibrary(usethis)\n\ncreate_from_github(\n  \"bcjaeger/melodem-apoe4-het\",\n  destdir = \"path/of/choice\", \n  fork = TRUE\n)"
  },
  {
    "objectID": "slides/01-introduction.html#introducing-targets",
    "href": "slides/01-introduction.html#introducing-targets",
    "title": "MELODEM data workshop",
    "section": "Introducing targets",
    "text": "Introducing targets"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn",
    "href": "slides/01-introduction.html#your-turn",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\n\nOpen _targets.R in the melodem-apoe4-het project.\nRun library(targets) to load the targets package.\nRun tar_load_globals() to load relevant functions and packages.\nRun tar_glimpse() to inspect the pipeline.\nRun tar_make() to make the pipeline.\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/01-introduction.html#start-with-data-management",
    "href": "slides/01-introduction.html#start-with-data-management",
    "title": "MELODEM data workshop",
    "section": "Start with data management",
    "text": "Start with data management\nIn the _targets.R file:\n\n\n\nfile_sim_tar &lt;- tar_target(\n  file_sim,\n  command = \"data/sim-raw.csv\",\n  format = 'file'\n)\n\ndata_sim_tar &lt;- tar_target(\n  data_sim,\n  data_prepare(file_sim)\n)\n\n\n\n\n\nThis will be done with your data, too!"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-1",
    "href": "slides/01-introduction.html#your-turn-1",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\nWe are going to add your data to the pipeline, carefully.\n\nThink of a name for your data.\n\nExample name: regards\n\nSave a copy of your data in data/sensitive. The name of your file should be name-raw.csv or name-raw.sas7bdat, where name is your data’s name. E.g., regards-raw.csv\n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-2",
    "href": "slides/01-introduction.html#your-turn-2",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\nWe are going to add your data to the pipeline, carefully.\n\nSwitch from the R console to the terminal.\nVerify you have no uncommitted changes:\n\n\ngit status\n\nShould return “nothing to commit, working tree clean”\n\nCreate a new branch with git:\n\n\ngit branch -b regards\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-3",
    "href": "slides/01-introduction.html#your-turn-3",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\nWe are going to add your data to the pipeline, carefully.\n\n\n\nCopy/paste code shown here to _targets.R, just beneath the line that starts with # real data cohorts.\nReplace zzzz with the name of your data.\nSave the _targets.R file\nRun tar_make() in the R console.\n\n\n\nfile_zzzz_tar &lt;- tar_target(               \n  file_zzzz,\n  command = \"data/sensitive/zzzz-raw.csv\",\n  format = \"file\"\n)\n\ndata_zzzz_tar &lt;- tar_target(\n  data_zzzz,\n  data_prepare(\n    file_name = \"data/sensitive/zzzz-raw.csv\"\n  )\n)\n\n# don't forget to add these targets\n# to the targets list at the bottom!\n\n\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-4",
    "href": "slides/01-introduction.html#your-turn-4",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\n\nrun tar_read(data_zzzz), where zzzz is your data name.\n\n\ntar_read(data_sim)\n\n\n\n-------------------------------------- sim -------------------------------------- \n# A tibble: 1,000 × 8\n   time status apoe4       sex      age biomarker_1 biomarker_2 biomarker_3\n  &lt;dbl&gt;  &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 0.530      1 carrier     female  66.3      0.747      -0.742       -1.71 \n2 0.855      0 non_carrier female  65.5     -0.562      -0.0513       0.546\n3 0.120      1 carrier     male    71.4      0.0133     -0.184        0.826\n4 1.16       1 carrier     female  59.5      0.795      -1.32        -1.28 \n5 5.84       0 non_carrier female  58.5     -0.293       0.191        2.54 \n# ℹ 995 more rows\n\n ----------------------------------  exclusions  ---------------------------------- \n# A tibble: 1 × 2\n  label            n_obs\n  &lt;glue&gt;           &lt;int&gt;\n1 sim participants  1000"
  },
  {
    "objectID": "slides/01-introduction.html#data-management",
    "href": "slides/01-introduction.html#data-management",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nWe used data_prepare() to make this object.\nLet’s check out what data_prepare does.\n\ndata_prepare &lt;- function(file_name, ...){\n\n  output &lt;- data_load(file_name) %&gt;%\n    data_clean() %&gt;%\n    data_derive() %&gt;%\n    data_select() %&gt;%\n    data_recode(labels = labels) %&gt;%\n    data_exclude(...)\n  \n  # checks not shown\n\n  output\n\n}"
  },
  {
    "objectID": "slides/01-introduction.html#data-management-1",
    "href": "slides/01-introduction.html#data-management-1",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nWe used data_prepare() to make this object.\nLet’s check out what data_prepare does. First, it loads the data\n\ndata_prepare &lt;- function(file_name, ...){\n\n  output &lt;- data_load(file_name) %&gt;%\n    data_clean() %&gt;%\n    data_derive() %&gt;%\n    data_select() %&gt;%\n    data_exclude(...)\n\n  check_names(output$values,\n              c(\"age\", \"sex\", \"apoe4\", \"time\", \"status\"))\n\n  output\n\n}"
  },
  {
    "objectID": "slides/01-introduction.html#data-management-2",
    "href": "slides/01-introduction.html#data-management-2",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nLet’s check out data_load().\n\ndata_load &lt;- function(file_path){\n\n    # ... file management code not shown ...\n  \n  structure(\n    .Data = list(\n      values = data_input,\n      exclusions = tibble(label = glue(\"{cohort_name} participants\"),\n                          n_obs = nrow(data_input))\n    ),\n    class = c(paste(\"melodem\", cohort_name, sep = \"_\"), \n              'melodem_data'),\n    label = cohort_label\n  )\n  \n}"
  },
  {
    "objectID": "slides/01-introduction.html#data-management-3",
    "href": "slides/01-introduction.html#data-management-3",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nLet’s check out data_load(). The object returned from this function includes data and a preliminary exclusion table.\n\ndata_load &lt;- function(file_path){\n\n    # ... file management code not shown ...\n  \n  structure(\n    .Data = list(\n      values = data_input,\n      exclusions = tibble(label = glue(\"{cohort_name} participants\"),\n                          n_obs = nrow(data_input))\n    ),\n    class = c(cohort_name, 'melodem_data'),\n    label = cohort_label\n  )\n  \n}"
  },
  {
    "objectID": "slides/01-introduction.html#data-management-4",
    "href": "slides/01-introduction.html#data-management-4",
    "title": "MELODEM data workshop",
    "section": "Data management",
    "text": "Data management\nThe object returned also has customized class based on the dataset. The output also belongs to a broader class called melodem_data\n\ndata_load &lt;- function(file_path){\n\n  # ... file management code not shown ...\n  \n  structure(\n    .Data = list(\n      values = data_input,\n      exclusions = tibble(label = glue(\"{cohort_name} participants\"),\n                          n_obs = nrow(data_input))\n    ),\n    class = c(cohort_name, 'melodem_data'),\n    label = cohort_label\n  )\n  \n}"
  },
  {
    "objectID": "slides/01-introduction.html#why",
    "href": "slides/01-introduction.html#why",
    "title": "MELODEM data workshop",
    "section": "Why?",
    "text": "Why?\nEach dataset is unique, and some may require customized preparation:\n\nDifferent elements need to be cleaned.\nDifferent variables need to be derived.\nDifferent variables may be selected.\nDifferent exclusions may be applied.\n\ndata_load makes its output have a customized class based on the name of the dataset so that you, the owner of the data, are in control of these steps that may be uniquely defined for your data."
  },
  {
    "objectID": "slides/01-introduction.html#how",
    "href": "slides/01-introduction.html#how",
    "title": "MELODEM data workshop",
    "section": "How?",
    "text": "How?\nR’s generic function system. Generic functions (e.g., plot()) dispatch different methods depending on the type of input object.\n\nHere’s a look at the generic function for cleaning an object of class sim:\n\ndata_clean.melodem_sim &lt;- function(data){\n\n  dt &lt;- data_clean_minimal(data$values)\n\n  dt[, age := age * 5 + 65]\n  dt[, sex := fifelse(sex &gt; 0, 1, 0)]\n  dt[, sex := factor(sex, levels = c(0, 1),\n                     labels = c(\"male\", \"female\"))]\n  data$values &lt;- dt\n  data\n}"
  },
  {
    "objectID": "slides/01-introduction.html#how-1",
    "href": "slides/01-introduction.html#how-1",
    "title": "MELODEM data workshop",
    "section": "How?",
    "text": "How?\nR’s generic function system. Generic functions (e.g., plot()) dispatch different methods depending on the type of input object.\n\nHere’s the generic function for cleaning an object of class melodem_data:\n\ndata_clean.melodem_data &lt;- function(data){\n\n  data_clean_minimal(data)\n\n}"
  },
  {
    "objectID": "slides/01-introduction.html#your-data",
    "href": "slides/01-introduction.html#your-data",
    "title": "MELODEM data workshop",
    "section": "Your data",
    "text": "Your data\n\nYour data’s first class is melodem plus the name you picked, e.g., melodem_regards, and second class is melodem_data. Verify by running class()\nWhen you run data_clean() with, e.g., the regards data,\n\nR will look for a function called data_clean.melodem_regards\nIf it doesn’t exist, R runs data_clean.melodem_data\n\n\nTLDR: If you don’t write a specific function for data_clean, data_derive, etc. for your data, then these functions will not do anything to your data."
  },
  {
    "objectID": "slides/01-introduction.html#your-turn-5",
    "href": "slides/01-introduction.html#your-turn-5",
    "title": "MELODEM data workshop",
    "section": "Your turn",
    "text": "Your turn\nFor the rest of this session,\n\nImplement specific data_clean, data_derive, data_select, and data_exclude for your data.\nIf you already did these operations before the workshop, move the code you used into the corresponding function.\nIf you finish early, help someone else!\n\n\n\nSlides available at https://bcjaeger.github.io/melodem-apoe4-het/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "",
    "text": "These are the materials for the MEthods in LOngitudinal DEMentia (MELODEM) workshops offered at Château de Bellinglise, France, July 2024.\nThe workshop focuses on the use of random forests to predict dementia risk and identify heterogeneity in the association between APOE4 and dementia risk. This website hosts the materials for the workshop and the corresponding GitHub repository (todo: add link here) includes additional materials for manuscript development."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "",
    "text": "These are the materials for the MEthods in LOngitudinal DEMentia (MELODEM) workshops offered at Château de Bellinglise, France, July 2024.\nThe workshop focuses on the use of random forests to predict dementia risk and identify heterogeneity in the association between APOE4 and dementia risk. This website hosts the materials for the workshop and the corresponding GitHub repository (todo: add link here) includes additional materials for manuscript development."
  },
  {
    "objectID": "index.html#data-set-requirements",
    "href": "index.html#data-set-requirements",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Data set requirements",
    "text": "Data set requirements\nData sets must have the following characteristics:\n\na right-censored time-to-dementia outcome.\ndata on APOE4 (carrier vs non-carrier).\ndata on age and sex.\nYou personally are able to access it from the venue in France (e.g., Wifi, hard drive)\n\nIf the dataset doesn’t meet all of these criteria, it will not be eligible for the workshop analysis.\nFor efficiency during the workshop, please ensure the names of the required variables are as follows:\n\nTime to dementia should be named ‘time’ and should be numeric time in years from baseline assessment to the time of censoring, death, or dementia, whichever occurred first.\nDementia status should be named ‘status’ and should take values of 0 or 1, with 1 occurring if and only if dementia occurred before censoring and death.\nApoe4 should be named ‘apoe4’ and should have values of ‘carrier’ vs. ‘non_carrier’. Keep this binary for the sake of using it in causal random forests.\nAge should be named ‘age’ and should be numeric with age in years.\nSex should be named ‘sex’ and should include values of ‘male’ or ‘female’. This does not need to be binary if your data have a large sample (i.e., &gt; 250 observations with &gt;10 dementia events) of people who do not identify in either of those two categories\n\nData sets should have as many predictors as possible. Predictors would include any variable that\n\nMay be associated with dementia risk\nMay be associated with APOE4 status\nIs not an effect of dementia or cognitive impairment\nMay explain heterogeneity in the association of APOE4 with dementia risk.\n\nData sets need not have an exhaustive set of predictors, but any predictors must be measured before or at the start of follow-up for the right-censored dementia outcome. Because the analysis will be exploratory and focus on the identifying heterogeneity in the association of APOE4 with incident dementia, we encourage everyone to include as many valid predictors from their data as feasible. A few examples include (but are not limited to):\n\neducation,\nrace/ethnicity,\nsmoking history,\nmid-life htn,\nmid-life obesity,\nhtn drugs and statins,\nSES,\ndiabetes,\nrenal disease,\natrial fibrillation,\ncardiovascular disease,\ndepression,\nhead trauma,\ninsomnia,\northostatic hypotension,\nfamily history of cognitive impairment,\nfamily history of Alzheimer’s Disease"
  },
  {
    "objectID": "index.html#preparation",
    "href": "index.html#preparation",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Preparation",
    "text": "Preparation\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://posit.co/download/rstudio-desktop/\nA recent version of git, available at https://git-scm.com/downloads\nThe following R packages, which you can install from the R console:\n\n\n# A few more packages may be added - this list isn't final yet.\n# Install required packages for the workshop. \npkgs &lt;- \n  c(\"tidyverse\", \"tidymodels\", \"data.table\", \"haven\", \"magrittr\",\n    \"glue\", \"grf\", \"aorsf\", \"glmnet\", \"xgboost\", \"randomForestSRC\",\n    \"party\", \"riskRegression\", \"survival\", \"officer\", \"flextable\", \n    \"table.glue\", \"gtsummary\", \"usethis\", \"cli\", \"ggforce\",\n    \"rpart\", \"rpart.plot\", \"ranger\", \"withr\", \"gt\", \"recipes\", \n    \"butcher\", \"sandwich\", \"lmtest\", \"gbm\", \"officedown\", \"Matrix\",\n    \"ggsurvfit\", \"tidycmprsk\", \"here\", \"tarchetypes\", \"targets\",\n    \"palmerpenguins\", \"ggrepel\")\n\ninstall.packages('job')\n\njob::job({install.packages(pkgs)})\n\nIf you’re a Windows user and encounter an error message during installation noting a missing Rtools installation, install Rtools using the installer linked here.\n\nGitHub\nA major aim of the workshop will be to establish a baseline of collaboration for developing and publishing a manuscript. We will use GitHub to facilitate this.\n\nIf you do not have a GitHub account, please create one here: https://github.com/\nIf you have a GitHub account, make sure you have a personal access token for HTTPS stored in your local Rstudio. Instructions to do this are provided here: https://happygitwithr.com/https-pat\n\n\n\ntargets\nIn addition to using GitHub, we will also use targets to coordinate our workflow. There will be a brief tutorial for targets during the workshop, but getting familiar with this R package before the workshop is highly encouraged. A full textbook on targets is available: https://books.ropensci.org/targets/.\nReading the following sections prior to the workshop will be very helpful:\n\nIntroduction: https://books.ropensci.org/targets/#intro\nA walk through: https://books.ropensci.org/targets/walkthrough.html\nFunction-based workflow: https://books.ropensci.org/targets/functions.html"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participants’ convenience. They are not meant as standalone learning materials.\n\nDay 1: Oblique and causal random forests\n\nIntroduction\nDecision trees and random forests\nOblique random forests\nCausal random forests\n\n\n\nDay 2: Collaborative analysis and manuscript planning"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto and is based on the fantastic workshops developed by the tidymodels team. Please submit an issue (todo: add link to github repo) on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "index.html#reuse-and-licensing",
    "href": "index.html#reuse-and-licensing",
    "title": "Methods in longitudinal dementia research workshop",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e. not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "slides/04-causal_forests.html",
    "href": "slides/04-causal_forests.html",
    "title": "Causal random forests with grf",
    "section": "",
    "text": "Robinson’s residual-on-residual regression\n\nCausal trees\nCausal random forest\nCausal random survival forest\n\nInference with causal random forests\n\nConditional average treatment effect (CATE)\nBest linear projection (BLP)\nRank weighted average treatment effect (RATE)"
  },
  {
    "objectID": "slides/04-causal_forests.html#overview",
    "href": "slides/04-causal_forests.html#overview",
    "title": "Causal random forests with grf",
    "section": "",
    "text": "Robinson’s residual-on-residual regression\n\nCausal trees\nCausal random forest\nCausal random survival forest\n\nInference with causal random forests\n\nConditional average treatment effect (CATE)\nBest linear projection (BLP)\nRank weighted average treatment effect (RATE)"
  },
  {
    "objectID": "slides/04-causal_forests.html#the-partially-linear-model",
    "href": "slides/04-causal_forests.html#the-partially-linear-model",
    "title": "Causal random forests with grf",
    "section": "The partially linear model",
    "text": "The partially linear model\nSuppose\n\\[\nY_i = \\tau W_i + f(X_i) + \\varepsilon_i\n\\] Assume:\n\n\\(E[\\varepsilon_i | X_i, W_i] = 0\\)\nuntreated outcome is given by unknown function \\(f\\),\na treatment assignment shifts the outcome by \\(\\tau\\)."
  },
  {
    "objectID": "slides/04-causal_forests.html#how-to-estimate-tau",
    "href": "slides/04-causal_forests.html#how-to-estimate-tau",
    "title": "Causal random forests with grf",
    "section": "How to estimate \\(\\tau\\)?",
    "text": "How to estimate \\(\\tau\\)?\nSuppose\n\\[\nY_i = \\tau W_i + f(X_i) + \\varepsilon_i\n\\]\nHow do we estimate \\(\\tau\\) when we do not know \\(f(X_i)\\)?\nDefine:\n\\[\\begin{align*}\n\ne(x) &= E[W_i | X_i=x] \\,\\, \\text{(Propensity score)} \\\\\n\nm(x) &= E[Y_i | X_i = x] = f(x) + \\tau e(x) \\,\\,\\,\\,\\, \\text{(Cndl. mean of } Y\\text{)}\n\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#use-propensity-and-conditional-mean",
    "href": "slides/04-causal_forests.html#use-propensity-and-conditional-mean",
    "title": "Causal random forests with grf",
    "section": "Use propensity and conditional mean",
    "text": "Use propensity and conditional mean\nRe-express the partial linear model in terms of \\(e(x)\\) and \\(m(x)\\):\n\\[\\begin{align*}\n\nY_i &= \\tau W_i + f(X_i) + \\varepsilon_i, \\,  \\\\\n\nY_i - \\tau e (x) &= \\tau W_i + f(X_i) - \\tau e(x) + \\varepsilon_i, \\, \\\\\n\nY_i - f(X_i) - \\tau e (x) &= \\tau W_i - \\tau e (x) + \\varepsilon_i, \\, \\\\\n\nY_i - m(x) &= \\tau (W_i - e(x)) + \\varepsilon_i, \\, \\\\\n\n\\end{align*}\\]\n\\(\\tau\\) can be estimated with residual-on-residual regression (Robinson 1988).\nHow? Plug in flexible estimates of \\(m(x)\\) and \\(e(x)\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#re-write-as-a-linear-model",
    "href": "slides/04-causal_forests.html#re-write-as-a-linear-model",
    "title": "Causal random forests with grf",
    "section": "Re-write as a linear model",
    "text": "Re-write as a linear model\nMore formally,\n\\[\n\\hat{\\tau} := \\text{lm}\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i)\\Biggr).\n\\]\n\nSuperscript \\(^i\\) denotes cross-fit estimates (Chernozhukov et al. 2018).\nCross-fitting: estimate something, e.g., \\(e(x)\\), using cross-validation.\nWhy? removes bias from over-fitting."
  },
  {
    "objectID": "slides/04-causal_forests.html#example",
    "href": "slides/04-causal_forests.html#example",
    "title": "Causal random forests with grf",
    "section": "Example",
    "text": "Example\n\nSuppose \\(Y_i = \\tau W_i + f(X_i) + \\epsilon_i\\)\n\n\\(\\tau\\) is 1/2\n\\(W_i\\) is randomized treatment\n\\(X_i\\) is a continuous covariate\n\\(f(X_i) = |X_i|\\)\n\nBy defn, \\(E[W_i] = 1/2\\) (why?).\n\n\n\n# Set up for const tau example\n\nset.seed(1)\ntau_truth &lt;- 1/2\nn &lt;- 1000\n\n# randomized treatment\nW &lt;- rbinom(n = n, size = 1, prob = 1/2)\n\n# continuous covariate\nX &lt;- rnorm(n = n)\n\n# outcome \nY &lt;- tau_truth * W + abs(X) + rnorm(n)\n\ndata &lt;- data.frame(Y=Y, X=X, W=W)"
  },
  {
    "objectID": "slides/04-causal_forests.html#example-done-wrong",
    "href": "slides/04-causal_forests.html#example-done-wrong",
    "title": "Causal random forests with grf",
    "section": "Example done wrong",
    "text": "Example done wrong\n\nFirst we’ll do it the wrong way.\n\nFit a classical model to estimate conditional mean of \\(Y\\).\nCompute residuals and run Robinson’s regression.\nWhat’d we do wrong?\n\n\n\n\nlibrary(glue)\nlibrary(ggplot2)\n\nfit_cmean &lt;- lm(Y ~ X, data = data)\n\nm_x &lt;- predict(fit_cmean, new_data = data)\n\nresid_y &lt;- Y - m_x\nresid_w &lt;- W - 1/2\n\ntau_fit &lt;- lm(resid_y ~ resid_w) \n\nglue(\"True tau is {tau_truth}, \\\\\n      estimated tau is {coef(tau_fit)[2]}\")\n\nTrue tau is 0.5, estimated tau is 0.462441735905258"
  },
  {
    "objectID": "slides/04-causal_forests.html#conditional-mean-predictions",
    "href": "slides/04-causal_forests.html#conditional-mean-predictions",
    "title": "Causal random forests with grf",
    "section": "Conditional mean predictions…",
    "text": "Conditional mean predictions…"
  },
  {
    "objectID": "slides/04-causal_forests.html#example-done-wrong-take-2",
    "href": "slides/04-causal_forests.html#example-done-wrong-take-2",
    "title": "Causal random forests with grf",
    "section": "Example done wrong, take 2",
    "text": "Example done wrong, take 2\n\nThe model for conditional mean was under-specified.\n\nFit a flexible model to estimate conditional mean of \\(Y\\).\nCompute residuals and run Robinson’s regression.\nWhat’d we do wrong?\n\n\n\n\nlibrary(aorsf)\n\nfit_cmean &lt;- orsf(Y ~ X, data = data)\n\nm_x &lt;- predict(fit_cmean, new_data = data)\n\nresid_y &lt;- Y - m_x\nresid_w &lt;- W - 1/2\n\ntau_fit &lt;- lm(resid_y ~ resid_w) \n\nglue(\"True tau is {tau_truth}, \\\\\n      estimated tau is {coef(tau_fit)[2]}\")\n\nTrue tau is 0.5, estimated tau is 0.394643899391371"
  },
  {
    "objectID": "slides/04-causal_forests.html#example-done-right",
    "href": "slides/04-causal_forests.html#example-done-right",
    "title": "Causal random forests with grf",
    "section": "Example done right",
    "text": "Example done right\n\nWe forgot about cross-fitting!\n\nFit a flexible model to estimate conditional mean of \\(Y\\).\nUse out-of-bag predictions.\nCompute residuals and run Robinson’s regression.\n\n\n\n\nm_x_oobag &lt;- predict(fit_cmean, oobag = TRUE)\n\nresid_y &lt;- Y - m_x_oobag\nresid_w &lt;- W - 1/2\n\ntau_fit &lt;- lm(resid_y ~ resid_w) \n\nglue(\"True tau is {tau_truth}, \\\\\n      estimated tau is {coef(tau_fit)[2]}\")\n\nTrue tau is 0.5, estimated tau is 0.495509155402123"
  },
  {
    "objectID": "slides/04-causal_forests.html#conditional-mean-predictions-1",
    "href": "slides/04-causal_forests.html#conditional-mean-predictions-1",
    "title": "Causal random forests with grf",
    "section": "Conditional mean predictions",
    "text": "Conditional mean predictions"
  },
  {
    "objectID": "slides/04-causal_forests.html#how-to-grow-causal-trees",
    "href": "slides/04-causal_forests.html#how-to-grow-causal-trees",
    "title": "Causal random forests with grf",
    "section": "How to grow causal trees",
    "text": "How to grow causal trees\nCausal trees are much like standard decision trees, but they maximize\n\\[n_L \\cdot n_R \\cdot (\\hat{\\tau}_L-\\hat{\\tau}_R)^2\\]\nwhere residual-on-residual regression is used to estimate \\(\\hat{\\tau}_L\\) and \\(\\hat{\\tau}_R\\)\n\ngrf estimates \\(\\hat \\tau\\) once in the parent node and uses “influence functions” to approximate how \\(\\hat\\tau\\) would change if an observation moved from one child node to the other (Wager and Athey 2018).\nPredictions from leaves are \\(E[Y|W=1] - E[Y|W=0]\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#how-to-grow-causal-trees-1",
    "href": "slides/04-causal_forests.html#how-to-grow-causal-trees-1",
    "title": "Causal random forests with grf",
    "section": "How to grow causal trees",
    "text": "How to grow causal trees\nCausal trees use “honesty” and “subsampling” (Wager and Athey 2018).\n\nHonesty: Each training observation is used for one of the following:\n\nEstimate the treatment effect for leaf nodes.\nDecide splitting values for non-leaf nodes.\n\nSubsampling: While Breiman (2001)’s random forest uses bootstrap sampling with replacement, the causal random forest samples without replacement."
  },
  {
    "objectID": "slides/04-causal_forests.html#back-to-the-partial-linear-model",
    "href": "slides/04-causal_forests.html#back-to-the-partial-linear-model",
    "title": "Causal random forests with grf",
    "section": "Back to the partial linear model",
    "text": "Back to the partial linear model\nRelaxing the assumption of a constant treatment:\n\\[\nY_i = \\color{red}{\\tau(X_i)} W_i + f(X_i) + \\varepsilon_i, \\,\n\\]\nwhere \\(\\color{red}{\\tau(X_i)}\\) is the conditional average treatment (CATE). If we had a neighborhood \\(\\mathcal{N}(x)\\) where \\(\\tau\\) was constant, then we could do residual-on-residual regression in the neighborhood:\n\\[\n\\hat\\tau_i(x) := lm\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), \\color{red}{w = 1\\{X_i \\in \\mathcal{N}(x) \\}}\\Biggr),\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forest-adaptive-neighborhoods",
    "href": "slides/04-causal_forests.html#random-forest-adaptive-neighborhoods",
    "title": "Causal random forests with grf",
    "section": "Random forest adaptive neighborhoods",
    "text": "Random forest adaptive neighborhoods\nSuppose we fit a random forest with \\(B\\) trees to a training set of size \\(n\\), and we compute a prediction \\(p\\) for a new observation \\(x\\):\n\\[\\begin{equation*}\n\\begin{split}\n& p = \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|}\n\\end{split}\n\\end{equation*}\\]\n\n\\(L_b(x)\\) indicates the leaf node that \\(x\\) falls into for tree \\(b\\)\nThe inner sum is the mean of outcomes in the same leaf as \\(x\\)\nThis generalizes to causal random forests (it’s easier to write with regression trees)."
  },
  {
    "objectID": "slides/04-causal_forests.html#random-forest-adaptive-neighborhoods-1",
    "href": "slides/04-causal_forests.html#random-forest-adaptive-neighborhoods-1",
    "title": "Causal random forests with grf",
    "section": "Random forest adaptive neighborhoods",
    "text": "Random forest adaptive neighborhoods\nPull \\(Y_i\\) out of the sum that depends on \\(b\\):\n\\[\\begin{equation*}\n\\begin{split}\np &= \\sum_{i=1}^{n} \\frac{1}{B} \\sum_{b=1}^{B} Y_i \\frac{1\\{Xi \\in L_b(x)\\}} {|L_b(x)|} \\\\\n&= \\sum_{i=1}^{n} Y_i \\sum_{b=1}^{B} \\frac{1\\{Xi \\in L_b(x)\\}} {B \\cdot |L_b(x)|} \\\\\n& = \\sum_{i=1}^{n} Y_i \\color{blue}{\\alpha_i(x)},\n\\end{split}\n\\end{equation*}\\]\n\n\\(\\alpha_i(x) \\propto\\) no. of times observation \\(i\\) lands in the same leaf as \\(x\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#plug-weights-in-to-lm",
    "href": "slides/04-causal_forests.html#plug-weights-in-to-lm",
    "title": "Causal random forests with grf",
    "section": "Plug weights in to lm",
    "text": "Plug weights in to lm\nInstead of defining neighborhood boundaries, weight by similarity:\n\\[\n\\hat\\tau_i(x) := \\text{lm}\\Biggl( Y_i - \\hat m^{(-i)}(X_i) \\sim W_i - \\hat e^{(-i)}(X_i), w = \\color{blue}{\\alpha_i(x)} \\Biggr).\n\\] This forest-localized version of Robinson’s regression, paired with honesty and subsampling, gives asymptotic guarantees for estimation and inference (Wager and Athey 2018):\n\nPointwise consistency for the true treatment effect.\nAsymptotically Gaussian and centered sampling distribution."
  },
  {
    "objectID": "slides/04-causal_forests.html#three-main-components",
    "href": "slides/04-causal_forests.html#three-main-components",
    "title": "Causal random forests with grf",
    "section": "Three main components",
    "text": "Three main components\nThe procedure to estimate \\(\\hat\\tau_i\\) has three pieces:\n\\[\n\\hat\\tau_i(x) := \\text{lm}\\Biggl( Y_i - \\color{green}{\\hat m^{(-i)}(X_i)} \\sim W_i - \\color{red}{\\hat e^{(-i)}(X_i)}, w = \\color{blue}{\\alpha_i(x)} \\Biggr).\n\\]\n\n\\(\\color{green}{\\hat m^{(-i)}(X_i)}\\) is a flexible, cross-fit estimate for \\(E[Y|X]\\)\n\\(\\color{red}{\\hat e^{(-i)}(X_i)}\\) is a flexible, cross-fit estimate for \\(E[W|X]\\)\n\\(\\color{blue}{\\alpha_i(x)}\\) are the similarity weights from a causal random forest"
  },
  {
    "objectID": "slides/04-causal_forests.html#set-up",
    "href": "slides/04-causal_forests.html#set-up",
    "title": "Causal random forests with grf",
    "section": "Set up",
    "text": "Set up\nAssume the survival setting:\n\\[\\begin{equation}\n  Y_i =\n    \\begin{cases}\n      T_i & \\text{if } \\, T_i \\leq C_i \\\\\n      C_i & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\\]\nWhere \\(T_i\\) is time to event and (\\(C_i\\)) is time to censoring. Define\n\\[\\begin{equation}\nD_i =\n    \\begin{cases}\n      1 & \\text{if } \\, T_i \\leq C_i \\\\\n      0 & \\text{otherwise.}\n    \\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#observed-time-versus-true-time",
    "href": "slides/04-causal_forests.html#observed-time-versus-true-time",
    "title": "Causal random forests with grf",
    "section": "Observed time versus true time",
    "text": "Observed time versus true time\n\nEvent times are obscured by\n\ncensoring\nend of follow-up, i.e., \\(h\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#observed-time-versus-true-time-1",
    "href": "slides/04-causal_forests.html#observed-time-versus-true-time-1",
    "title": "Causal random forests with grf",
    "section": "Observed time versus true time",
    "text": "Observed time versus true time\n\nEvent times are obscured by\n\ncensoring\nend of follow-up, i.e., \\(h\\)\n\nEstimate restricted mean survival time (RMST): \\(E \\left[ \\text{min}(T, h) \\right]\\). See Cui et al. (2023) for more details on adjustment for censoring."
  },
  {
    "objectID": "slides/04-causal_forests.html#treatment-effects-for-survival",
    "href": "slides/04-causal_forests.html#treatment-effects-for-survival",
    "title": "Causal random forests with grf",
    "section": "Treatment effects for survival",
    "text": "Treatment effects for survival\nTwo treatment effects can be estimated conditional on \\(h\\).\n\nRMST \\[\\tau(x) = E[\\min(T(1), h) - \\min(T(0), h) \\, | X = x],\\]\nSurvival probability: \\[\\tau(x) = P[T(1) &gt; h \\, | X = x] - P[T(0) &gt; h \\, | X = x].\\] \\(T(1)\\) and \\(T(0)\\) are treated and untreated event times, respectively."
  },
  {
    "objectID": "slides/04-causal_forests.html#summaries-of-cates",
    "href": "slides/04-causal_forests.html#summaries-of-cates",
    "title": "Causal random forests with grf",
    "section": "Summaries of CATEs",
    "text": "Summaries of CATEs\nYou could compute average treatment effect (ATE) as the mean of CATEs:\n\\[\\hat\\tau = \\frac{1}{n}\\sum_{i=1}^n \\hat\\tau_i(x)\\] But the augmented inverse probability weighted ATE is better:\n\\[\n\\hat \\tau_{AIPW} = \\frac{1}{n} \\sum_{i=1}^{n}\\left( \\overbrace{\\tau(X_i)}^{\\text{Initial estimate}} + \\overbrace{\\frac{W_i - e(X_i)}{e(X_i)[1 - e(X_i)]}}^{\\text{debiasing weight}} \\cdot \\overbrace{\\left(Y_i - \\mu(X_i, W_i)\\right)}^{\\text{residual}} \\right)\n\\]"
  },
  {
    "objectID": "slides/04-causal_forests.html#summaries-of-cates-contd.",
    "href": "slides/04-causal_forests.html#summaries-of-cates-contd.",
    "title": "Causal random forests with grf",
    "section": "Summaries of CATEs contd.",
    "text": "Summaries of CATEs contd.\nFor simplicity, re-write the augmented inverse probability ATE as\n\\[\\hat\\tau = \\frac{1}{n}\\sum_{i=1}^n \\hat\\Gamma_i(x),\\] With a vector of these \\(\\Gamma_i\\)’s, define:\n\nAverage treatment effect (ATE) = mean(gamma)\nBest linear projection (BLP) = lm(gamma ~ X)"
  },
  {
    "objectID": "slides/04-causal_forests.html#estimating-the-ate",
    "href": "slides/04-causal_forests.html#estimating-the-ate",
    "title": "Causal random forests with grf",
    "section": "Estimating the ATE",
    "text": "Estimating the ATE\nFirst, we’ll prepare the data:\n\n# loads packages and R functions\ntargets::tar_load_globals()\n\n# loads a specific target\ntar_load(data_melodem)\n\nSecond, coerce data to grf format:\n\n# helper function for grf data prep \ndata_grf &lt;- data_coerce_grf(data_melodem$values)\n\n# just a view of the X matrix\nhead(data_grf$X, n = 2)\n\n         statin    aspirin      egfr    sub_ckd        age    sub_cvd\n[1,] -0.8994367 -1.0374288 0.8063765 -0.5983451 -1.4119699 -0.4897294\n[2,]  1.1116507  0.9637867 0.4961700 -0.5983451 -0.8576657 -0.4897294\n            CHR        GLUR         HDL        TRR     UMALCR        BMI\n[1,]  0.4445567  0.07162673 -0.04571026 -0.6775012 -0.1185727  0.3706294\n[2,] -0.2114133 -0.67907349 -0.67458295  1.0027111 -0.1936573 -1.0884217\n             sbp        dbp fr_risk10yrs  orth_hypo   education\n[1,] -0.40504337 0.58627338   -0.7134939 -0.2760562 -0.07626949\n[2,]  0.05247806 0.05612061   -0.3414115 -0.2760562 -0.07626949\n     frailty_catg_Frail frailty_catg_Pre.frail frailty_catg_unknown\n[1,]                  0                      1                    0\n[2,]                  0                      1                    0\n     race4_HISPANIC race4_OTHER race4_WHITE race4_unknown sex_female\n[1,]              0           0           0             0          1\n[2,]              0           0           1             0          1\n     sex_unknown\n[1,]           0\n[2,]           0"
  },
  {
    "objectID": "slides/04-causal_forests.html#fitting-the-forest",
    "href": "slides/04-causal_forests.html#fitting-the-forest",
    "title": "Causal random forests with grf",
    "section": "Fitting the forest",
    "text": "Fitting the forest\nThird, fit the causal survival forest:\n\nfit_grf &lt;- causal_survival_forest(\n  X = data_grf$X, # covariates\n  Y = data_grf$Y, # time to event\n  W = data_grf$W, # treatment status\n  D = data_grf$D, # event status\n  horizon = 3, # 3-year horizon\n  # treatment effect will be\n  # measured in terms of the\n  # restricted mean survival time\n  target = 'RMST' \n)\n\nfit_grf\n\nGRF forest object of type causal_survival_forest \nNumber of trees: 2000 \nNumber of training samples: 7159 \nVariable importance: \n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n0.003 0.002 0.064 0.000 0.117 0.012 0.067 0.060 0.047 0.033 0.154 0.117 0.027 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n0.083 0.112 0.006 0.050 0.021 0.001 0.000 0.020 0.000 0.005 0.000 0.001 0.000"
  },
  {
    "objectID": "slides/04-causal_forests.html#get-gamma-scores",
    "href": "slides/04-causal_forests.html#get-gamma-scores",
    "title": "Causal random forests with grf",
    "section": "Get \\(\\Gamma\\) scores",
    "text": "Get \\(\\Gamma\\) scores\nRemember the \\(\\Gamma_i\\)’s that provide conditional estimates of \\(\\tau\\)? Let’s get them.\n\n# pull the augmented CATEs from the fitted grf object\ngammas &lt;- get_scores(fit_grf)\n\n\nWith gammas, we can compute ATE manually\n\n\nmean(gammas) \n\n[1] -0.001035183\n\n\n\nVerify this is what the grf function gives\n\n\naverage_treatment_effect(fit_grf)\n\n    estimate      std.err \n-0.001035183  0.002502688"
  },
  {
    "objectID": "slides/04-causal_forests.html#your-turn",
    "href": "slides/04-causal_forests.html#your-turn",
    "title": "Causal random forests with grf",
    "section": "Your turn",
    "text": "Your turn\nOpen classwork/04-causal_forests.qmd and complete Exercise 1\n\nReminder: Pink sticky note for help, blue sticky when you finish.\nNote: the data_coerce_grf() function can save you lots of time.\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/04-causal_forests.html#estimating-the-blp",
    "href": "slides/04-causal_forests.html#estimating-the-blp",
    "title": "Causal random forests with grf",
    "section": "Estimating the BLP",
    "text": "Estimating the BLP\nThe BLP (Semenova and Chernozhukov 2021):\n\nIs estimated by regressing a set of covariates on \\(\\Gamma\\).\nCan be estimated for a subset of covariates\nCan be estimated for a subset of observations.\nSummarizes heterogeneous treatment effects conditional on covariates.\n\nYou can estimate BLP manually:\n\ndata_blp &lt;- bind_cols(gamma = gammas, data_grf$X)\n\nfit_blp &lt;- lm(gamma ~ ., data = data_blp)"
  },
  {
    "objectID": "slides/04-causal_forests.html#what-happens-underneath-the-grf-hood",
    "href": "slides/04-causal_forests.html#what-happens-underneath-the-grf-hood",
    "title": "Causal random forests with grf",
    "section": "What happens underneath the grf hood",
    "text": "What happens underneath the grf hood\nHere’s how you can replicate grf results:\n\n\nlmtest::coeftest(fit_blp, \n                 vcov = sandwich::vcovCL, \n                 type = 'HC3')\n\n\nt test of coefficients:\n\n                          Estimate  Std. Error t value Pr(&gt;|t|)\n(Intercept)             0.00439932  0.00632648  0.6954   0.4868\nstatin                 -0.00213497  0.00287177 -0.7434   0.4572\naspirin                 0.00314203  0.00281955  1.1144   0.2652\negfr                    0.00040709  0.00330155  0.1233   0.9019\nsub_ckd                 0.00292551  0.00369910  0.7909   0.4290\nage                     0.00393674  0.00477607  0.8243   0.4098\nsub_cvd                -0.00423127  0.00310080 -1.3646   0.1724\nCHR                     0.00312453  0.00412521  0.7574   0.4488\nGLUR                   -0.00380898  0.00272365 -1.3985   0.1620\nHDL                     0.00101039  0.00350525  0.2882   0.7732\nTRR                     0.00173615  0.00296643  0.5853   0.5584\nUMALCR                 -0.00324024  0.00245118 -1.3219   0.1862\nBMI                    -0.00205123  0.00257303 -0.7972   0.4254\nsbp                     0.00015949  0.00378842  0.0421   0.9664\ndbp                     0.00324218  0.00410429  0.7899   0.4296\nfr_risk10yrs           -0.00351058  0.00654447 -0.5364   0.5917\north_hypo               0.00090846  0.00265606  0.3420   0.7323\neducation              -0.00474135  0.00331396 -1.4307   0.1526\nfrailty_catg_Frail     -0.00949440  0.00874185 -1.0861   0.2775\nfrailty_catg_Pre.frail -0.00305295  0.00501740 -0.6085   0.5429\nrace4_HISPANIC         -0.00548466  0.01196162 -0.4585   0.6466\nrace4_OTHER             0.02118089  0.01405513  1.5070   0.1319\nrace4_WHITE             0.00701133  0.00652185  1.0751   0.2824\nsex_female             -0.01388631  0.00873421 -1.5899   0.1119\n\n\n\n\n\nbest_linear_projection(fit_grf, data_grf$X)\n\n\nBest linear projection of the conditional average treatment effect.\nConfidence intervals are cluster- and heteroskedasticity-robust (HC3):\n\n                          Estimate  Std. Error t value Pr(&gt;|t|)\n(Intercept)             0.00439932  0.00632648  0.6954   0.4868\nstatin                 -0.00213497  0.00287177 -0.7434   0.4572\naspirin                 0.00314203  0.00281955  1.1144   0.2652\negfr                    0.00040709  0.00330155  0.1233   0.9019\nsub_ckd                 0.00292551  0.00369910  0.7909   0.4290\nage                     0.00393674  0.00477607  0.8243   0.4098\nsub_cvd                -0.00423127  0.00310080 -1.3646   0.1724\nCHR                     0.00312453  0.00412521  0.7574   0.4488\nGLUR                   -0.00380898  0.00272365 -1.3985   0.1620\nHDL                     0.00101039  0.00350525  0.2882   0.7732\nTRR                     0.00173615  0.00296643  0.5853   0.5584\nUMALCR                 -0.00324024  0.00245118 -1.3219   0.1862\nBMI                    -0.00205123  0.00257303 -0.7972   0.4254\nsbp                     0.00015949  0.00378842  0.0421   0.9664\ndbp                     0.00324218  0.00410429  0.7899   0.4296\nfr_risk10yrs           -0.00351058  0.00654447 -0.5364   0.5917\north_hypo               0.00090846  0.00265606  0.3420   0.7323\neducation              -0.00474135  0.00331396 -1.4307   0.1526\nfrailty_catg_Frail     -0.00949440  0.00874185 -1.0861   0.2775\nfrailty_catg_Pre.frail -0.00305295  0.00501740 -0.6085   0.5429\nrace4_HISPANIC         -0.00548466  0.01196162 -0.4585   0.6466\nrace4_OTHER             0.02118089  0.01405513  1.5070   0.1319\nrace4_WHITE             0.00701133  0.00652185  1.0751   0.2824\nsex_female             -0.01388631  0.00873421 -1.5899   0.1119"
  },
  {
    "objectID": "slides/04-causal_forests.html#your-turn-1",
    "href": "slides/04-causal_forests.html#your-turn-1",
    "title": "Causal random forests with grf",
    "section": "Your turn",
    "text": "Your turn\nComplete Exercise 2\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/04-causal_forests.html#rank-weighted-average-treatment-effect",
    "href": "slides/04-causal_forests.html#rank-weighted-average-treatment-effect",
    "title": "Causal random forests with grf",
    "section": "Rank-Weighted Average Treatment Effect",
    "text": "Rank-Weighted Average Treatment Effect\nWhile ATE and BLP are helpful, they do not tell us the following:\n\nHow good is a treatment prioritization rule at distinguishing sub-populations with different conditional treatment effects?\nIs there any heterogeneity present in a conditional treatment effect?\n\nThe rank-weighted average treatment effect (RATE) answers both of these."
  },
  {
    "objectID": "slides/04-causal_forests.html#treatment-prioritization-rules",
    "href": "slides/04-causal_forests.html#treatment-prioritization-rules",
    "title": "Causal random forests with grf",
    "section": "Treatment prioritization rules",
    "text": "Treatment prioritization rules\nSuppose we have a treatment that benefits some (not all) adults. Who should initiate treatment? A treatment prioritization rule can help:\n\nhigh score for those likely to benefit from treatment.\nlow score for those likely to have a small/negative benefit from treatment.\n\nRisk prediction models can be a treatment prioritization rule:\n\nInitiate antihypertensive medication if predicted risk for cardiovascular disease is high."
  },
  {
    "objectID": "slides/04-causal_forests.html#how-to-evaluate-treatment-prioritization",
    "href": "slides/04-causal_forests.html#how-to-evaluate-treatment-prioritization",
    "title": "Causal random forests with grf",
    "section": "How to evaluate treatment prioritization",
    "text": "How to evaluate treatment prioritization\nThe basic idea:\n\nChop the population up into subgroups based on the prioritization rule, e.g., by decile of score.\nEstimate the ATE in each group, separately, and compare to the overall estimated ATE from treating everyone\nPlot the difference between group-specific ATE and the overall ATE for each of the groups\n\nExample: the Targeting Operator Characteristic (TOC)"
  },
  {
    "objectID": "slides/04-causal_forests.html#targeting-operator-characteristic-toc",
    "href": "slides/04-causal_forests.html#targeting-operator-characteristic-toc",
    "title": "Causal random forests with grf",
    "section": "Targeting Operator Characteristic (TOC)",
    "text": "Targeting Operator Characteristic (TOC)\n\n\nCreate groups by including the top q\\(^\\text{th}\\) fraction of individuals with the largest prioritization score.\nUse many values of \\(q\\) to make the pattern more curve-like\nMotivation: Receiver Operating Characteristic (ROC) curve, a widely used metric for assessing discrimination of predictions."
  },
  {
    "objectID": "slides/04-causal_forests.html#rate-area-underneath-toc",
    "href": "slides/04-causal_forests.html#rate-area-underneath-toc",
    "title": "Causal random forests with grf",
    "section": "RATE: area underneath TOC",
    "text": "RATE: area underneath TOC\n\nRATE is estimated by taking the area underneath the TOC curve.\n\\[\\textrm{RATE} = \\int_0^1 \\textrm{TOC}(q) dq .\\]\nAs \\(\\tau(X_i)\\) approaches a constant, RATE approaches 0."
  },
  {
    "objectID": "slides/04-causal_forests.html#rate-of-prediction-model",
    "href": "slides/04-causal_forests.html#rate-of-prediction-model",
    "title": "Causal random forests with grf",
    "section": "RATE of prediction model",
    "text": "RATE of prediction model\nLet’s use RATE to see how well risk prediction works as a treatment prioritization rule.\n\nlibrary(aorsf)\n\nfit_orsf &lt;- orsf(time + status ~ .,\n                 data = data_melodem$values, \n                 na_action = 'impute_meanmode',\n                 oobag_pred_horizon = 3)\n\n# important to use oobag!\nprd_risk &lt;- as.numeric(fit_orsf$pred_oobag)\n\nprd_rate &lt;- \n  rank_average_treatment_effect(fit_grf, \n                                priorities = prd_risk)"
  },
  {
    "objectID": "slides/04-causal_forests.html#not-good",
    "href": "slides/04-causal_forests.html#not-good",
    "title": "Causal random forests with grf",
    "section": "Not good",
    "text": "Not good"
  },
  {
    "objectID": "slides/04-causal_forests.html#estimating-rate-from-cate",
    "href": "slides/04-causal_forests.html#estimating-rate-from-cate",
    "title": "Causal random forests with grf",
    "section": "Estimating RATE from CATE",
    "text": "Estimating RATE from CATE\nAn intuitive way to assign treatment priority is to use the CATE: \\(\\hat\\tau(X_i)\\)\n\n\\(\\hat\\tau(X_i)\\) should not be estimated and evaluated using the same data\nUse split-sample estimation or cross-fitting (Yadlowsky et al. 2021).\n\nAs a preliminary step, we’ll split our data in to training and testing sets\n\ntrain_index &lt;- sample(x = nrow(data_melodem$values), \n                      size = nrow(data_melodem$values)/2)\n\ndata_trn &lt;- data_melodem$values[train_index, ]\ndata_tst &lt;- data_melodem$values[-train_index, ]\n\ndata_trn_grf &lt;- data_coerce_grf(data_trn)\ndata_tst_grf &lt;- data_coerce_grf(data_tst)"
  },
  {
    "objectID": "slides/04-causal_forests.html#fitting",
    "href": "slides/04-causal_forests.html#fitting",
    "title": "Causal random forests with grf",
    "section": "Fitting",
    "text": "Fitting\nWe fit one forest with training data to estimate CATE and fit another forest with testing data to evaluate the CATE estimates:\n\n\nfit_trn_grf &lt;- causal_survival_forest(\n  X = data_trn_grf$X, Y = data_trn_grf$Y,\n  W = data_trn_grf$W, D = data_trn_grf$D,\n  horizon = 3, target = 'RMST' \n)\n\nfit_tst_grf &lt;- causal_survival_forest(\n  X = data_tst_grf$X, Y = data_tst_grf$Y,\n  W = data_tst_grf$W, D = data_tst_grf$D,\n  horizon = 3, target = 'RMST' \n)"
  },
  {
    "objectID": "slides/04-causal_forests.html#predicting",
    "href": "slides/04-causal_forests.html#predicting",
    "title": "Causal random forests with grf",
    "section": "Predicting",
    "text": "Predicting\nWe use the forest fitted to training data to estimate CATE for the testing data. For illustration, we also estimate naive CATE\n\n\n# the fitted forest hasn't\n# seen the testing data\ntau_hat_split &lt;- fit_trn_grf %&gt;% \n  predict(data_tst_grf$X) %&gt;% \n  getElement(\"predictions\")\n\n# Illustration only (don't do this)\ntau_hat_naive &lt;- fit_trn_grf %&gt;% \n  predict(data_trn_grf$X) %&gt;% \n  getElement(\"predictions\")"
  },
  {
    "objectID": "slides/04-causal_forests.html#evaluating",
    "href": "slides/04-causal_forests.html#evaluating",
    "title": "Causal random forests with grf",
    "section": "Evaluating",
    "text": "Evaluating\nWe use the forest fitted to the testing data to evaluate the CATE estimates for observations in the testing data\n\n\nrate_split &lt;- \n  rank_average_treatment_effect(\n    forest = fit_tst_grf, \n    priorities = tau_hat_split, \n    target = \"AUTOC\"\n  )\n\n# Illustration only (don't do this)\nrate_naive &lt;- \n  rank_average_treatment_effect(\n    forest = fit_trn_grf, \n    priorities = tau_hat_naive, \n    target = \"AUTOC\"\n  )"
  },
  {
    "objectID": "slides/04-causal_forests.html#correct-versus-overly-optimistic",
    "href": "slides/04-causal_forests.html#correct-versus-overly-optimistic",
    "title": "Causal random forests with grf",
    "section": "Correct versus overly optimistic",
    "text": "Correct versus overly optimistic\nThe problem with being overly optimistic is it has very high type 1 error"
  },
  {
    "objectID": "slides/04-causal_forests.html#your-turn-2",
    "href": "slides/04-causal_forests.html#your-turn-2",
    "title": "Causal random forests with grf",
    "section": "Your turn",
    "text": "Your turn\nComplete Exercise 4\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/04-causal_forests.html#to-the-pipeline",
    "href": "slides/04-causal_forests.html#to-the-pipeline",
    "title": "Causal random forests with grf",
    "section": "To the pipeline",
    "text": "To the pipeline\n\nCopy/paste this code into your _targets.R file.\n\n\n# in _targets.R, you should see this comment: \n\n# real data model targets (to be added as an exercise). \n\n# Paste this code right beneath that.\n\ngrf_shareable_zzzz_tar &lt;- tar_target(\n  grf_shareable_zzzz,\n  grf_summarize(fit_grf_zzzz)\n)\n\n\nModify this code, replacing zzzz with the name of your dataset."
  },
  {
    "objectID": "slides/02-trees_and_forests.html",
    "href": "slides/02-trees_and_forests.html",
    "title": "Decision trees and random forests",
    "section": "",
    "text": "Decision trees\n\nGrowing trees\nLeaf nodes\n\nRandom Forests\n\nOut-of-bag predictions\nVariable importance\n\n\n\n\n\nData were collected and made available by Dr. Kristen Gorman and the Palmer Station, a member of the Long Term Ecological Research Network."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#overview",
    "href": "slides/02-trees_and_forests.html#overview",
    "title": "Decision trees and random forests",
    "section": "Overview",
    "text": "Overview\n\nDecision trees\n\nGrowing trees\nLeaf nodes\n\nRandom Forests\n\nOut-of-bag predictions\nVariable importance"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#growing-decision-trees",
    "href": "slides/02-trees_and_forests.html#growing-decision-trees",
    "title": "Decision trees and random forests",
    "section": "Growing decision trees",
    "text": "Growing decision trees\n\nDecision trees are grown by recursively splitting a set of training data (Breiman 2017)."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#gini-impurity",
    "href": "slides/02-trees_and_forests.html#gini-impurity",
    "title": "Decision trees and random forests",
    "section": "Gini impurity",
    "text": "Gini impurity\n\nFor classification, “Gini impurity” measures split quality:\n\\[G = 1 - \\sum_{i = 1}^{K} P(i)^2\\]\n\\(K\\) is no. of classes, \\(P(i)\\) is the probability of class \\(i\\)."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#first-split-right-node",
    "href": "slides/02-trees_and_forests.html#first-split-right-node",
    "title": "Decision trees and random forests",
    "section": "First split: right node",
    "text": "First split: right node\n\nConsider splitting at flipper length of 206.5: Right node:\n\ngini_right &lt;- penguins %&gt;% \n filter(flipper_length_mm&gt;=206.5) %&gt;% \n count(species) %&gt;% \n mutate(p = n / sum(n)) %&gt;% \n summarize(gini = 1 - sum(p^2)) %&gt;% \n pull(gini)\n\ngini_right\n\n[1] 0.107008"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#first-split-left-node",
    "href": "slides/02-trees_and_forests.html#first-split-left-node",
    "title": "Decision trees and random forests",
    "section": "First split: left node",
    "text": "First split: left node\n\nConsider splitting at flipper length of 206.5: Left node:\n\ngini_left &lt;- penguins %&gt;% \n filter(flipper_length_mm&lt;206.5) %&gt;% \n count(species) %&gt;% \n mutate(p = n / sum(n)) %&gt;% \n summarize(gini = 1 - sum(p^2)) %&gt;% \n pull(gini)\n\ngini_left\n\n[1] 0.4289479"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#first-split-total-impurity",
    "href": "slides/02-trees_and_forests.html#first-split-total-impurity",
    "title": "Decision trees and random forests",
    "section": "First split: total impurity",
    "text": "First split: total impurity\n\nConsider splitting at flipper length of 206.5: Impurity:\n\nsplit_var &lt;- penguins %&gt;% \n  pull(flipper_length_mm)\n\nn_tot &lt;- length(split_var)\nn_right &lt;- sum(split_var &gt;= 206.5)\nn_left &lt;- sum(split_var &lt; 206.5)\n\ngini_right * (n_right / n_tot) + \n  gini_left * (n_left / n_tot)\n\n[1] 0.3080996"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#your-turn",
    "href": "slides/02-trees_and_forests.html#your-turn",
    "title": "Decision trees and random forests",
    "section": "Your turn",
    "text": "Your turn\nWas this the best possible split? Let’s find out.\n\nOpen classwork/02-trees_and_forests.qmd\nComplete exercise 1.\nReminder: pink sticky note if you’d like help, blue sticky note when you are finished.\nHint: the answer is 0.492340868530637\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#regression-trees",
    "href": "slides/02-trees_and_forests.html#regression-trees",
    "title": "Decision trees and random forests",
    "section": "Regression trees",
    "text": "Regression trees\n\nImpurity is based on variance:\n\\[\\sum_{i=1}^n \\frac{(y_i - \\bar{y})^2}{n-1}\\]\n\n\\(\\bar{y}\\): mean of \\(y_i\\) in the node.\n\\(n\\): no. of observations in the node."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#survival-trees",
    "href": "slides/02-trees_and_forests.html#survival-trees",
    "title": "Decision trees and random forests",
    "section": "Survival trees",
    "text": "Survival trees\n\nLog-rank stat measures split quality (Ishwaran et al. 2008):\n\\[\\frac{ \\sum_{j=1}^J O_{j } - E_{j}}{\\sqrt{\\sum_{j=1}^J V_{j}}}\\] \\(O\\) is observed and \\(E\\) expected events in the node at time \\(j\\). \\(V\\) is variance."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#keep-growing",
    "href": "slides/02-trees_and_forests.html#keep-growing",
    "title": "Decision trees and random forests",
    "section": "Keep growing?",
    "text": "Keep growing?\n\nNow we have two potential datasets, or nodes in the tree, that we can split.\nDo we keep going or stop?"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#stopping-conditions",
    "href": "slides/02-trees_and_forests.html#stopping-conditions",
    "title": "Decision trees and random forests",
    "section": "Stopping conditions",
    "text": "Stopping conditions\n\nWe may stop tree growth if the node has:\n\nObs &lt; min_obs\nCases &lt; min_cases\nImpurity &lt; min_impurity\nDepth = max_depth"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#your-turn-1",
    "href": "slides/02-trees_and_forests.html#your-turn-1",
    "title": "Decision trees and random forests",
    "section": "Your turn",
    "text": "Your turn\n\nSuppose\n\nmin_obs = 10\nmin_cases = 2\nmin_impurity = 0.2\nmax_depth = 3\n\nWhich node(s) can we split?"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#split-the-left-node",
    "href": "slides/02-trees_and_forests.html#split-the-left-node",
    "title": "Decision trees and random forests",
    "section": "Split the left node",
    "text": "Split the left node\n\n\nThe left node can be split.\nThe right node cannot, since impurity on the right is &lt; min_impurity"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#finished-growing",
    "href": "slides/02-trees_and_forests.html#finished-growing",
    "title": "Decision trees and random forests",
    "section": "Finished growing",
    "text": "Finished growing\n\n\nThe left node can be split.\nAfter splitting the left node, all nodes have impurity &lt; min_impurity.\nIf no more nodes to grow, convert partitioned sets into leaf nodes"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#leaves",
    "href": "slides/02-trees_and_forests.html#leaves",
    "title": "Decision trees and random forests",
    "section": "Leaves?",
    "text": "Leaves?\n\nWhat is a leaf node?\n\nA leaf node is a terminal node in the tree.\nPredictions for new data are stored in leaf nodes.\n\n\n\nmutate(\n  penguins,\n  node = case_when(\n    flipper_length_mm &gt;= 206.5 ~ 'leaf_1',\n    bill_length_mm &gt;= 43.35 ~ 'leaf_2',\n    TRUE ~ 'leaf_3'\n  )\n) %&gt;% \n  group_by(node) %&gt;% count(species) %&gt;% \n  mutate(n = n / sum(n)) %&gt;% \n  pivot_wider(values_from = n, names_from = species, \n              values_fill = 0)\n\n# A tibble: 3 × 4\n# Groups:   node [3]\n  node   Adelie Chinstrap Gentoo\n  &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 leaf_1 0.016     0.04   0.944 \n2 leaf_2 0.0635    0.921  0.0159\n3 leaf_3 0.966     0.0345 0"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#your-turn-2",
    "href": "slides/02-trees_and_forests.html#your-turn-2",
    "title": "Decision trees and random forests",
    "section": "Your turn",
    "text": "Your turn\nFit your own decision tree to the penguins data.\n\nOpen classwork/02-trees_and_forests.qmd\nComplete Exercise 2\n\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#from-partition-to-flowchart",
    "href": "slides/02-trees_and_forests.html#from-partition-to-flowchart",
    "title": "Decision trees and random forests",
    "section": "From partition to flowchart",
    "text": "From partition to flowchart\nThe same partitions, visualized as a binary tree.\n\nAs a reminder, here is our ‘hand-made’ leaf data\n\n\n\n\n\n\n\n\n\nAdelie\nChinstrap\nGentoo\n\n\n\n\nleaf_1\n0.02\n0.04\n0.94\n\n\nleaf_2\n0.06\n0.92\n0.02\n\n\nleaf_3\n0.97\n0.03\n0.00"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#leaves-for-other-types-of-trees",
    "href": "slides/02-trees_and_forests.html#leaves-for-other-types-of-trees",
    "title": "Decision trees and random forests",
    "section": "Leaves for other types of trees",
    "text": "Leaves for other types of trees\n\nFor regression, leaves store the mean value of the outcome.\nFor survival, leaves store:\n\ncumulative hazard function\nsurvival function\nsummary score (e.g., no. of expected events)"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#planting-seeds-for-causal-trees",
    "href": "slides/02-trees_and_forests.html#planting-seeds-for-causal-trees",
    "title": "Decision trees and random forests",
    "section": "Planting seeds for causal trees",
    "text": "Planting seeds for causal trees\nSuppose we have outcome \\(Y\\) and treatment \\(W\\in\\{0, 1\\}\\).\n\\[\\text{Define }\\tau = E[Y|W=1] - E[Y|W=0],\\]\nIf \\(\\tau\\) can be estimated conditional on a person’s characteristics, then a decision tree could be grown using \\(\\hat\\tau_1 \\mid x_1, \\ldots, \\hat\\tau_n \\mid x_n\\) as an outcome. That tree could predict who benefits from treatment and explain why.\n\nBut how do we get \\(\\hat\\tau_i \\mid x_i\\)? More on this later…"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#expert-or-committee",
    "href": "slides/02-trees_and_forests.html#expert-or-committee",
    "title": "Decision trees and random forests",
    "section": "Expert or committee?",
    "text": "Expert or committee?\nIf we have to make a yes/no decision, who should make it?\n\n\n1 expert who is right 75% of the time\nMajority rule by 5000 independent “weak experts”.\n\nNote: each weak expert is right 51% of the time.\n\nAnswer is weak experts!"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#why-committee",
    "href": "slides/02-trees_and_forests.html#why-committee",
    "title": "Decision trees and random forests",
    "section": "Why committee?",
    "text": "Why committee?\nIf we have to make a yes/no decision, who should make it?\n\n1 expert who is right 75% of the time\nMajority rule by 5000 independent “weak experts”.\n\nNote: each weak expert is right 51% of the time.\n\nAnswer is weak experts!\n\nWhy? Weak expert majority is right ~92% of the time\n\n# probability that &gt;2500 weak experts are right\n# = 1 - probability that &lt;=2500 are right\n1 - pbinom(q = 2500, size = 5000, prob = 0.51)\n\n[1] 0.9192858"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forest-recipe",
    "href": "slides/02-trees_and_forests.html#random-forest-recipe",
    "title": "Decision trees and random forests",
    "section": "Random forest recipe",
    "text": "Random forest recipe\nThe weak expert approach for decision trees (Breiman 2001):\n\nGrow each tree with a random subset of the training data.\nEvaluate a random subset of predictors when splitting data.\n\nThese random elements help make the trees more independent while keeping their prediction accuracy better than random guesses."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#a-single-traditional-tree-is-fine",
    "href": "slides/02-trees_and_forests.html#a-single-traditional-tree-is-fine",
    "title": "Decision trees and random forests",
    "section": "A single traditional tree is fine",
    "text": "A single traditional tree is fine"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#a-single-randomized-tree-struggles",
    "href": "slides/02-trees_and_forests.html#a-single-randomized-tree-struggles",
    "title": "Decision trees and random forests",
    "section": "A single randomized tree struggles",
    "text": "A single randomized tree struggles"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#five-randomized-trees-do-okay",
    "href": "slides/02-trees_and_forests.html#five-randomized-trees-do-okay",
    "title": "Decision trees and random forests",
    "section": "Five randomized trees do okay",
    "text": "Five randomized trees do okay"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#randomized-trees-do-great",
    "href": "slides/02-trees_and_forests.html#randomized-trees-do-great",
    "title": "Decision trees and random forests",
    "section": "100 randomized trees do great",
    "text": "100 randomized trees do great"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#randomized-trees---no-overfitting",
    "href": "slides/02-trees_and_forests.html#randomized-trees---no-overfitting",
    "title": "Decision trees and random forests",
    "section": "500 randomized trees - no overfitting!",
    "text": "500 randomized trees - no overfitting!"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#random-forest-predictions",
    "href": "slides/02-trees_and_forests.html#random-forest-predictions",
    "title": "Decision trees and random forests",
    "section": "Random forest predictions",
    "text": "Random forest predictions\nTo get a prediction from the random forest with \\(B\\) trees,\n\npredict the outcome with each tree\ntake the mean of the predictions\n\n\\[\\hat{y}_i = \\frac{1}{B}\\sum_{b=1}^{B} \\hat{y}_b(i)\\]"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag",
    "href": "slides/02-trees_and_forests.html#out-of-bag",
    "title": "Decision trees and random forests",
    "section": "Out-of-bag",
    "text": "Out-of-bag\nBreiman (1996) discusses “out-of-bag” estimation.\n\n‘bag’: the bootstrapped sample used to grow a tree\n‘out-of-bag’: the observations that were not in the bootstrap sample\nout-of-bag prediction error: nearly optimal estimate of the forest’s external prediction error\n\nWith conventional bootstrap sampling, each observation has about a 36.8% chance of being out-of-bag for each tree."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#how-to-get-out-of-bag-predictions",
    "href": "slides/02-trees_and_forests.html#how-to-get-out-of-bag-predictions",
    "title": "Decision trees and random forests",
    "section": "How to get out-of-bag predictions",
    "text": "How to get out-of-bag predictions"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#grow-tree-1",
    "href": "slides/02-trees_and_forests.html#grow-tree-1",
    "title": "Decision trees and random forests",
    "section": "Grow tree #1",
    "text": "Grow tree #1"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#store-its-predictions-and-denominators",
    "href": "slides/02-trees_and_forests.html#store-its-predictions-and-denominators",
    "title": "Decision trees and random forests",
    "section": "Store its predictions and denominators",
    "text": "Store its predictions and denominators"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#now-do-tree-2",
    "href": "slides/02-trees_and_forests.html#now-do-tree-2",
    "title": "Decision trees and random forests",
    "section": "Now do tree #2",
    "text": "Now do tree #2"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#accumulate-predictions-and-denominator",
    "href": "slides/02-trees_and_forests.html#accumulate-predictions-and-denominator",
    "title": "Decision trees and random forests",
    "section": "Accumulate predictions and denominator",
    "text": "Accumulate predictions and denominator"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#out-of-bag-predictions",
    "href": "slides/02-trees_and_forests.html#out-of-bag-predictions",
    "title": "Decision trees and random forests",
    "section": "Out-of-bag predictions",
    "text": "Out-of-bag predictions\nFinal out-of-bag predictions:\n\\[\\hat{y}_i^{\\text{oob}} = \\frac{1}{|B_i|}\\sum_{b \\in B_i}\\hat{y}_b\\]\n\n\\(B_i\\) is the set of trees where \\(i\\) is out of bag\n\nOut-of-bag estimation plays a pivotal role in variable importance estimates and inference with causal random forests."
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance",
    "href": "slides/02-trees_and_forests.html#variable-importance",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\n\nFirst compute out-of-bag prediction accuracy.\nHere, classification accuracy is 96%"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance-1",
    "href": "slides/02-trees_and_forests.html#variable-importance-1",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\n\nNext, permute the values of a given variable\nSee how one value of flipper length is permuted in the figure?"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance-2",
    "href": "slides/02-trees_and_forests.html#variable-importance-2",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\n\nNext, permute the values of a given variable\nNow they are all permuted, and out-of-bag classification accuracy is now 61%"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance-3",
    "href": "slides/02-trees_and_forests.html#variable-importance-3",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\n\nRinse and repeat for all variables.\nFor bill length, out-of-bag classification accuracy is reduced to 63%"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#variable-importance-4",
    "href": "slides/02-trees_and_forests.html#variable-importance-4",
    "title": "Decision trees and random forests",
    "section": "Variable importance",
    "text": "Variable importance\nOnce all variables have been through this process:\n\\[\\text{Variable importance} = \\text{initial accuracy} - \\text{permuted accuracy}\\]\n\nFlipper length: 0.96 - 0.61 = 0.35\nBill length: 0.96 - 0.63 = 0.33\n\n\\(\\Rightarrow\\) flippers more important than bills for species prediction.\nNote: Details on limitations in Strobl et al. (2007)"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#your-turn-3",
    "href": "slides/02-trees_and_forests.html#your-turn-3",
    "title": "Decision trees and random forests",
    "section": "Your turn",
    "text": "Your turn\nComplete exercise 3.\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/02-trees_and_forests.html#references",
    "href": "slides/02-trees_and_forests.html#references",
    "title": "Decision trees and random forests",
    "section": "References",
    "text": "References\n\n\nBreiman, Leo. 1996. “Out-of-Bag Estimation.”\n\n\n———. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\n———. 2017. “Classification and Regression Trees.”\n\n\nIshwaran, Hemant, Udaya B Kogalur, Eugene H Blackstone, and Michael S Lauer. 2008. “Random Survival Forests.”\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. 2007. “Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution.” BMC Bioinformatics 8: 1–21.\n\n\n\n\n\nSlides available at https://bcjaeger.github.io/melodem-apoe4-het/"
  },
  {
    "objectID": "slides/03-oblique_forests.html",
    "href": "slides/03-oblique_forests.html",
    "title": "Oblique random forests with aorsf",
    "section": "",
    "text": "What does oblique mean?\naorsf\n\nMotivation\nDesign\nBenchmarks\nApplications"
  },
  {
    "objectID": "slides/03-oblique_forests.html#overview",
    "href": "slides/03-oblique_forests.html#overview",
    "title": "Oblique random forests with aorsf",
    "section": "",
    "text": "What does oblique mean?\naorsf\n\nMotivation\nDesign\nBenchmarks\nApplications"
  },
  {
    "objectID": "slides/03-oblique_forests.html#background",
    "href": "slides/03-oblique_forests.html#background",
    "title": "Oblique random forests with aorsf",
    "section": "Background",
    "text": "Background\nAxis-based splits (left) and oblique splits (right)"
  },
  {
    "objectID": "slides/03-oblique_forests.html#oblique-tree-first-split",
    "href": "slides/03-oblique_forests.html#oblique-tree-first-split",
    "title": "Oblique random forests with aorsf",
    "section": "Oblique tree: first split",
    "text": "Oblique tree: first split\n\nFirst, split mostly by bill length"
  },
  {
    "objectID": "slides/03-oblique_forests.html#oblique-tree-second-split",
    "href": "slides/03-oblique_forests.html#oblique-tree-second-split",
    "title": "Oblique random forests with aorsf",
    "section": "Oblique tree: second split",
    "text": "Oblique tree: second split\n\nFirst, split mostly by bill length\n\nSecond, make a triangle for the gentoo."
  },
  {
    "objectID": "slides/03-oblique_forests.html#oblique-random-forests",
    "href": "slides/03-oblique_forests.html#oblique-random-forests",
    "title": "Oblique random forests with aorsf",
    "section": "Oblique random forests",
    "text": "Oblique random forests\n\nAggregating randomized trees gives the oblique random forest"
  },
  {
    "objectID": "slides/03-oblique_forests.html#surprisingly-different",
    "href": "slides/03-oblique_forests.html#surprisingly-different",
    "title": "Oblique random forests with aorsf",
    "section": "Surprisingly different!",
    "text": "Surprisingly different!\n\nDespite very many similarities, axis-based and oblique random forests may give different results."
  },
  {
    "objectID": "slides/03-oblique_forests.html#prior-benchmarks",
    "href": "slides/03-oblique_forests.html#prior-benchmarks",
    "title": "Oblique random forests with aorsf",
    "section": "Prior benchmarks",
    "text": "Prior benchmarks\n\nBreiman (2001) found oblique random forests compared more favorably to boosting than axis based ones.\nMenze et al. (2011) coined the term ‘oblique’ random forest and introduced variable importance metrics for it.\nOn benchmarking 190 classifiers on 121 public datasets, Katuwal, Suganthan, and Zhang (2020) found variations on the oblique random forests were the top 3 classifiers.\n\n\nLeo Breiman named it “Forest-RC” but it later came to be known as oblique\n\nYet, everyone uses axis-based random forests. Few people even know that oblique random forests exist."
  },
  {
    "objectID": "slides/03-oblique_forests.html#heres-why",
    "href": "slides/03-oblique_forests.html#heres-why",
    "title": "Oblique random forests with aorsf",
    "section": "Here’s why:",
    "text": "Here’s why:"
  },
  {
    "objectID": "slides/03-oblique_forests.html#what-is-the-problem",
    "href": "slides/03-oblique_forests.html#what-is-the-problem",
    "title": "Oblique random forests with aorsf",
    "section": "What is the problem?",
    "text": "What is the problem?\nOblique random forests are slow. How slow, you ask?\n\nlibrary(ODRF)\nlibrary(ranger)\nlibrary(microbenchmark)\n\nmicrobenchmark(\n  ODRF = ODRF(species ~ bill_length_mm + flipper_length_mm, \n              data = penguins, \n              ntrees = 500),\n  ranger = ranger(species ~ bill_length_mm + flipper_length_mm,\n                  data = penguins,\n                  num.trees = 500),\n  times = 5\n)\n\nUnit: milliseconds\n   expr       min        lq      mean    median        uq       max neval cld\n   ODRF 4651.9503 4757.0415 4779.5978 4789.0610 4845.8683 4854.0679     5  a \n ranger   20.2193   20.7901   23.5078   21.8179   23.3712   31.3405     5   b"
  },
  {
    "objectID": "slides/03-oblique_forests.html#why-so-slow",
    "href": "slides/03-oblique_forests.html#why-so-slow",
    "title": "Oblique random forests with aorsf",
    "section": "Why so slow?",
    "text": "Why so slow?\nSuppose we’re given \\(p\\) continuous predictors, each with \\(k\\) unique values.\n\nIf predictor has \\(k\\) unique values \\(\\Rightarrow\\) max \\(k-2\\) potential splits\nAt most, \\(p \\cdot (k-2)\\) axis-based splits to assess\n\nSuppose we assess \\(L\\) linear combinations of predictors:\n\nAt most, \\(L \\cdot (k-2)\\) oblique splits to assess\n\nUsually, \\(L \\gg p\\) to find good oblique splits"
  },
  {
    "objectID": "slides/03-oblique_forests.html#find-1-good-linear-combination",
    "href": "slides/03-oblique_forests.html#find-1-good-linear-combination",
    "title": "Oblique random forests with aorsf",
    "section": "Find 1 good linear combination?",
    "text": "Find 1 good linear combination?\nWhat if we use a method like regression to find \\(L=1\\) linear combination?\n\\[\\text{Time to assess an oblique split }= R + S\\]\nWhere\n\n\\(R\\) is time to fit regression\n\\(S\\) is time to assess up to \\(k-2\\) splits\n\nIf \\(R\\) is small, we can do okay with this!"
  },
  {
    "objectID": "slides/03-oblique_forests.html#but-r-is-not-small",
    "href": "slides/03-oblique_forests.html#but-r-is-not-small",
    "title": "Oblique random forests with aorsf",
    "section": "But \\(R\\) is not small",
    "text": "But \\(R\\) is not small\nIt takes \\(\\approx\\) 2.5ms for 1 oblique split, $$20ms to fit 500 axis-based trees…\n\n# compare time for 1 oblique split to 500 axis-based trees\nmicrobenchmark(\n  # 1 oblique split\n  glm = glm(I(species == 'Adelie') ~ bill_length_mm + flipper_length_mm,\n            family = 'binomial', \n            data = penguins),\n  # 500 axis based trees\n  ranger = ranger(species ~ bill_length_mm + flipper_length_mm,\n                  data = penguins,\n                  num.trees = 500)\n)\n\nUnit: milliseconds\n   expr     min       lq      mean   median      uq     max neval cld\n    glm  1.7714  2.07825  2.778256  2.47365  2.5821 23.4204   100  a \n ranger 20.6938 21.58195 21.989103 21.85165 22.0588 40.8994   100   b\n\n\n1 oblique tree with 10 splits takes longer than 500 axis-based trees 😩"
  },
  {
    "objectID": "slides/03-oblique_forests.html#aorsf",
    "href": "slides/03-oblique_forests.html#aorsf",
    "title": "Oblique random forests with aorsf",
    "section": "aorsf",
    "text": "aorsf\nJaeger et al. (2022): Accelerated oblique random (survival) forest\n\nWe take the approach of minimizing \\(R\\)\nInstead of fitting a full regression model, we\n\nAvoid scaling during regression (minimize data copying)\nRelax convergence criteria (e.g., use just 1 iteration)\n\n\nWhile model convergence is vital for valid statistical inference, it isn’t that big of a deal for finding oblique splits. Remember, we just have to do slightly better than random guessing."
  },
  {
    "objectID": "slides/03-oblique_forests.html#benchmark",
    "href": "slides/03-oblique_forests.html#benchmark",
    "title": "Oblique random forests with aorsf",
    "section": "Benchmark",
    "text": "Benchmark\nMinimizing \\(R\\) helps.\n\nmicrobenchmark(\n  ODRF = ODRF(species ~ bill_length_mm + flipper_length_mm, \n              data = penguins, \n              ntrees = 500),\n  aorsf = orsf(species ~ bill_length_mm + flipper_length_mm, \n              data = penguins, \n              n_tree = 500),\n  ranger = ranger(species ~ bill_length_mm + flipper_length_mm,\n                  data = penguins,\n                  num.trees = 500),\n  times = 5\n)\n\nUnit: milliseconds\n   expr       min        lq       mean    median        uq       max neval cld\n   ODRF 4559.8742 4767.7961 4745.46626 4773.5435 4802.6260 4823.4915     5  a \n  aorsf   46.4154   46.4625   55.92320   51.6300   52.4436   82.6645     5   b\n ranger   21.6600   21.8220   22.42138   22.0821   23.0740   23.4688     5   b"
  },
  {
    "objectID": "slides/03-oblique_forests.html#more-benchmarks",
    "href": "slides/03-oblique_forests.html#more-benchmarks",
    "title": "Oblique random forests with aorsf",
    "section": "More benchmarks",
    "text": "More benchmarks\nJaeger et al. (2024) compare aorsf to obliqueRSF:\n\nEvaluated in 35 risk prediction tasks (21 datasets)\nMeasured computation time and C-statistic.\nUsed Bayesian linear mixed models to test for differences."
  },
  {
    "objectID": "slides/03-oblique_forests.html#computation-time",
    "href": "slides/03-oblique_forests.html#computation-time",
    "title": "Oblique random forests with aorsf",
    "section": "Computation time",
    "text": "Computation time\naorsf over 300 times faster than obliqueRSF"
  },
  {
    "objectID": "slides/03-oblique_forests.html#c-statistic",
    "href": "slides/03-oblique_forests.html#c-statistic",
    "title": "Oblique random forests with aorsf",
    "section": "C-statistic",
    "text": "C-statistic\naorsf practically equivalent to obliqueRSF"
  },
  {
    "objectID": "slides/03-oblique_forests.html#data",
    "href": "slides/03-oblique_forests.html#data",
    "title": "Oblique random forests with aorsf",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "slides/03-oblique_forests.html#data-1",
    "href": "slides/03-oblique_forests.html#data-1",
    "title": "Oblique random forests with aorsf",
    "section": "Data",
    "text": "Data\nI’ll use Alzheimer’s disease data (ad_data) from the modeldata package.\n\nlibrary(modeldata)\nglimpse(ad_data, width = 100)\n\nRows: 333\nColumns: 131\n$ ACE_CD143_Angiotensin_Converti   &lt;dbl&gt; 2.0031003, 1.5618560, 1.5206598, 1.6808260, 2.4009308, 0.…\n$ ACTH_Adrenocorticotropic_Hormon  &lt;dbl&gt; -1.3862944, -1.3862944, -1.7147984, -1.6094379, -0.967584…\n$ AXL                              &lt;dbl&gt; 1.09838668, 0.68328157, -0.14527630, 0.68328157, 0.190890…\n$ Adiponectin                      &lt;dbl&gt; -5.360193, -5.020686, -5.809143, -5.115996, -4.779524, -5…\n$ Alpha_1_Antichymotrypsin         &lt;dbl&gt; 1.7404662, 1.4586150, 1.1939225, 1.2809338, 2.1282317, 1.…\n$ Alpha_1_Antitrypsin              &lt;dbl&gt; -12.631361, -11.909882, -13.642963, -15.523564, -11.13306…\n$ Alpha_1_Microglobulin            &lt;dbl&gt; -2.577022, -3.244194, -2.882404, -3.170086, -2.343407, -2…\n$ Alpha_2_Macroglobulin            &lt;dbl&gt; -72.65029, -154.61228, -136.52918, -98.36175, -144.94460,…\n$ Angiopoietin_2_ANG_2             &lt;dbl&gt; 1.06471074, 0.74193734, 0.83290912, 0.91629073, 0.9555114…\n$ Angiotensinogen                  &lt;dbl&gt; 2.510547, 2.457283, 1.976365, 2.376085, 2.862219, 2.52402…\n$ Apolipoprotein_A_IV              &lt;dbl&gt; -1.427116, -1.660731, -1.660731, -2.120264, -1.171183, -1…\n$ Apolipoprotein_A1                &lt;dbl&gt; -7.402052, -7.047017, -7.684284, -8.047190, -6.725434, -7…\n$ Apolipoprotein_A2                &lt;dbl&gt; -0.26136476, -0.86750057, -0.65392647, -1.23787436, 0.095…\n$ Apolipoprotein_B                 &lt;dbl&gt; -4.624044, -6.747507, -3.976069, -6.517424, -3.378594, -2…\n$ Apolipoprotein_CI                &lt;dbl&gt; -1.2729657, -1.2729657, -1.7147984, -1.9661129, -0.755022…\n$ Apolipoprotein_CIII              &lt;dbl&gt; -2.312635, -2.343407, -2.748872, -2.995732, -1.514128, -2…\n$ Apolipoprotein_D                 &lt;dbl&gt; 2.0794415, 1.3350011, 1.3350011, 1.4350845, 1.6292405, 1.…\n$ Apolipoprotein_E                 &lt;dbl&gt; 3.7545215, 3.0971187, 2.7530556, 2.3713615, 3.0671471, 0.…\n$ Apolipoprotein_H                 &lt;dbl&gt; -0.15734908, -0.57539617, -0.34483937, -0.53172814, 0.662…\n$ B_Lymphocyte_Chemoattractant_BL  &lt;dbl&gt; 2.2969819, 1.6731213, 1.6731213, 1.9805094, 2.2969819, 2.…\n$ BMP_6                            &lt;dbl&gt; -2.200744, -1.728053, -2.062421, -1.982912, -1.241520, -1…\n$ Beta_2_Microglobulin             &lt;dbl&gt; 0.69314718, 0.47000363, 0.33647224, 0.64185389, 0.3364722…\n$ Betacellulin                     &lt;int&gt; 34, 53, 49, 52, 67, 51, 41, 42, 58, 59, 32, 43, 51, 53, 4…\n$ C_Reactive_Protein               &lt;dbl&gt; -4.074542, -6.645391, -8.047190, -6.214608, -4.342806, -7…\n$ CD40                             &lt;dbl&gt; -0.7964147, -1.2733760, -1.2415199, -1.1238408, -0.924034…\n$ CD5L                             &lt;dbl&gt; 0.09531018, -0.67334455, 0.09531018, -0.32850407, 0.36331…\n$ Calbindin                        &lt;dbl&gt; 33.21363, 25.27636, 22.16609, 23.45584, 21.83275, 13.2315…\n$ Calcitonin                       &lt;dbl&gt; 1.3862944, 3.6109179, 2.1162555, -0.1508229, 1.3083328, 1…\n$ CgA                              &lt;dbl&gt; 397.6536, 465.6759, 347.8639, 334.2346, 442.8046, 137.947…\n$ Clusterin_Apo_J                  &lt;dbl&gt; 3.555348, 3.044522, 2.772589, 2.833213, 3.044522, 2.56494…\n$ Complement_3                     &lt;dbl&gt; -10.36305, -16.10824, -16.10824, -13.20556, -12.81314, -1…\n$ Complement_Factor_H              &lt;dbl&gt; 3.5737252, 3.6000471, 4.4745686, 3.0971187, 7.2451496, 3.…\n$ Connective_Tissue_Growth_Factor  &lt;dbl&gt; 0.5306283, 0.5877867, 0.6418539, 0.5306283, 0.9162907, 0.…\n$ Cortisol                         &lt;dbl&gt; 10.0, 12.0, 10.0, 14.0, 11.0, 13.0, 4.9, 13.0, 12.0, 17.0…\n$ Creatine_Kinase_MB               &lt;dbl&gt; -1.710172, -1.751002, -1.383559, -1.647864, -1.625834, -1…\n$ Cystatin_C                       &lt;dbl&gt; 9.041922, 9.067624, 8.954157, 9.581904, 8.977146, 7.83597…\n$ EGF_R                            &lt;dbl&gt; -0.1354543, -0.3700474, -0.7329871, -0.4218532, -0.620603…\n$ EN_RAGE                          &lt;dbl&gt; -3.688879, -3.816713, -4.755993, -2.937463, -2.364460, -3…\n$ ENA_78                           &lt;dbl&gt; -1.349543, -1.356595, -1.390672, -1.367775, -1.339440, -1…\n$ Eotaxin_3                        &lt;int&gt; 53, 62, 62, 44, 64, 57, 64, 64, 64, 70, 82, 73, 70, 67, 3…\n$ FAS                              &lt;dbl&gt; -0.08338161, -0.52763274, -0.63487827, -0.47803580, -0.12…\n$ FSH_Follicle_Stimulation_Hormon  &lt;dbl&gt; -0.6516715, -1.6272839, -1.5630004, -0.5902871, -0.976300…\n$ Fas_Ligand                       &lt;dbl&gt; 3.1014922, 2.9788133, 1.3600098, 2.5372201, 4.0372847, 2.…\n$ Fatty_Acid_Binding_Protein       &lt;dbl&gt; 2.5208712, 2.2477966, 0.9063009, 0.6237306, 2.6345883, 0.…\n$ Ferritin                         &lt;dbl&gt; 3.329165, 3.932959, 3.176872, 3.138093, 2.690416, 1.84707…\n$ Fetuin_A                         &lt;dbl&gt; 1.2809338, 1.1939225, 1.4109870, 0.7419373, 2.1517622, 1.…\n$ Fibrinogen                       &lt;dbl&gt; -7.035589, -8.047190, -7.195437, -7.799353, -6.980326, -6…\n$ GRO_alpha                        &lt;dbl&gt; 1.381830, 1.372438, 1.412679, 1.372438, 1.398431, 1.39843…\n$ Gamma_Interferon_induced_Monokin &lt;dbl&gt; 2.949822, 2.721793, 2.762231, 2.885476, 2.851987, 2.82244…\n$ Glutathione_S_Transferase_alpha  &lt;dbl&gt; 1.0641271, 0.8670202, 0.8890150, 0.7083677, 1.2358607, 1.…\n$ HB_EGF                           &lt;dbl&gt; 6.559746, 8.754531, 7.745463, 5.949436, 7.245150, 6.41301…\n$ HCC_4                            &lt;dbl&gt; -3.036554, -4.074542, -3.649659, -3.816713, -3.146555, -3…\n$ Hepatocyte_Growth_Factor_HGF     &lt;dbl&gt; 0.58778666, 0.53062825, 0.09531018, 0.40546511, 0.5306282…\n$ I_309                            &lt;dbl&gt; 3.433987, 3.135494, 2.397895, 3.367296, 3.761200, 2.70805…\n$ ICAM_1                           &lt;dbl&gt; -0.1907787, -0.4620172, -0.4620172, -0.8572661, 0.0971503…\n$ IGF_BP_2                         &lt;dbl&gt; 5.609472, 5.347108, 5.181784, 5.424950, 5.420535, 5.05624…\n$ IL_11                            &lt;dbl&gt; 5.121987, 4.936704, 4.665910, 6.223931, 7.070709, 6.10321…\n$ IL_13                            &lt;dbl&gt; 1.282549, 1.269463, 1.274133, 1.307549, 1.309980, 1.28254…\n$ IL_16                            &lt;dbl&gt; 4.192081, 2.876338, 2.616102, 2.441056, 4.736472, 2.67103…\n$ IL_17E                           &lt;dbl&gt; 5.731246, 6.705891, 4.149327, 4.695848, 4.204987, 3.63705…\n$ IL_1alpha                        &lt;dbl&gt; -6.571283, -8.047190, -8.180721, -7.600902, -6.943657, -8…\n$ IL_3                             &lt;dbl&gt; -3.244194, -3.912023, -4.645992, -4.268698, -2.995732, -3…\n$ IL_4                             &lt;dbl&gt; 2.484907, 2.397895, 1.824549, 1.481605, 2.708050, 1.20896…\n$ IL_5                             &lt;dbl&gt; 1.09861229, 0.69314718, -0.24846136, 0.78845736, 1.163150…\n$ IL_6                             &lt;dbl&gt; 0.26936976, 0.09622438, 0.18568645, -0.37116408, -0.07204…\n$ IL_6_Receptor                    &lt;dbl&gt; 0.64279595, 0.43115645, 0.09668586, 0.57519641, 0.0966858…\n$ IL_7                             &lt;dbl&gt; 4.8050453, 3.7055056, 1.0056222, 2.3362105, 4.2875620, 2.…\n$ IL_8                             &lt;dbl&gt; 1.711325, 1.675557, 1.691393, 1.719944, 1.764298, 1.70827…\n$ IP_10_Inducible_Protein_10       &lt;dbl&gt; 6.242223, 5.686975, 5.049856, 5.602119, 6.369901, 5.48063…\n$ IgA                              &lt;dbl&gt; -6.812445, -6.377127, -6.319969, -7.621105, -4.645992, -5…\n$ Insulin                          &lt;dbl&gt; -0.6258253, -0.9431406, -1.4466191, -1.4852687, -0.300311…\n$ Kidney_Injury_Molecule_1_KIM_1   &lt;dbl&gt; -1.204295, -1.197703, -1.191191, -1.231557, -1.163800, -1…\n$ LOX_1                            &lt;dbl&gt; 1.7047481, 1.5260563, 1.1631508, 1.2237754, 1.3609766, 0.…\n$ Leptin                           &lt;dbl&gt; -1.5290628, -1.4660558, -1.6622675, -1.2693924, -0.915106…\n$ Lipoprotein_a                    &lt;dbl&gt; -4.268698, -4.933674, -5.843045, -4.990833, -2.937463, -4…\n$ MCP_1                            &lt;dbl&gt; 6.740519, 6.849066, 6.767343, 6.781058, 6.722630, 6.54103…\n$ MCP_2                            &lt;dbl&gt; 1.9805094, 1.8088944, 0.4005958, 1.9805094, 2.2208309, 2.…\n$ MIF                              &lt;dbl&gt; -1.237874, -1.897120, -2.302585, -1.660731, -1.897120, -2…\n$ MIP_1alpha                       &lt;dbl&gt; 4.968453, 3.690160, 4.049508, 4.928562, 6.452764, 4.60342…\n$ MIP_1beta                        &lt;dbl&gt; 3.258097, 3.135494, 2.397895, 3.218876, 3.526361, 2.89037…\n$ MMP_2                            &lt;dbl&gt; 4.478566, 3.781473, 2.866631, 2.968511, 3.690160, 2.91776…\n$ MMP_3                            &lt;dbl&gt; -2.207275, -2.465104, -2.302585, -1.771957, -1.560648, -3…\n$ MMP10                            &lt;dbl&gt; -3.270169, -3.649659, -2.733368, -4.074542, -2.617296, -3…\n$ MMP7                             &lt;dbl&gt; -3.7735027, -5.9681907, -4.0302269, -6.8561489, -0.222222…\n$ Myoglobin                        &lt;dbl&gt; -1.89711998, -0.75502258, -1.38629436, -1.13943428, -1.77…\n$ NT_proBNP                        &lt;dbl&gt; 4.553877, 4.219508, 4.248495, 4.110874, 4.465908, 4.18965…\n$ NrCAM                            &lt;dbl&gt; 5.003946, 5.209486, 4.744932, 4.969813, 5.198497, 3.25809…\n$ Osteopontin                      &lt;dbl&gt; 5.356586, 6.003887, 5.017280, 5.768321, 5.693732, 4.73619…\n$ PAI_1                            &lt;dbl&gt; 1.00350156, -0.03059880, 0.43837211, 0.00000000, 0.252304…\n$ PAPP_A                           &lt;dbl&gt; -2.902226, -2.813276, -2.935541, -2.786601, -2.935541, -2…\n$ PLGF                             &lt;dbl&gt; 4.442651, 4.025352, 4.510860, 3.433987, 4.795791, 4.39444…\n$ PYY                              &lt;dbl&gt; 3.218876, 3.135494, 2.890372, 2.833213, 3.663562, 3.33220…\n$ Pancreatic_polypeptide           &lt;dbl&gt; 0.57878085, 0.33647224, -0.89159812, -0.82098055, 0.26236…\n$ Prolactin                        &lt;dbl&gt; 0.00000000, -0.51082562, -0.13926207, -0.04082199, 0.1823…\n$ Prostatic_Acid_Phosphatase       &lt;dbl&gt; -1.620527, -1.739232, -1.636682, -1.739232, -1.696685, -1…\n$ Protein_S                        &lt;dbl&gt; -1.784998, -2.463991, -2.259135, -2.703458, -1.659842, -2…\n$ Pulmonary_and_Activation_Regulat &lt;dbl&gt; -0.8439701, -2.3025851, -1.6607312, -1.1086626, -0.562118…\n$ RANTES                           &lt;dbl&gt; -6.214608, -6.938214, -6.645391, -5.991465, -6.319969, -6…\n$ Resistin                         &lt;dbl&gt; -16.475315, -16.025283, -16.475315, -13.501240, -11.09283…\n$ S100b                            &lt;dbl&gt; 1.5618560, 1.7566212, 1.4357282, 1.2543998, 1.3012972, 1.…\n$ SGOT                             &lt;dbl&gt; -0.94160854, -0.65392647, 0.33647224, -0.19845094, 0.0953…\n$ SHBG                             &lt;dbl&gt; -1.897120, -1.560648, -2.207275, -3.146555, -2.430418, -2…\n$ SOD                              &lt;dbl&gt; 5.609472, 5.814131, 5.723585, 5.771441, 5.655992, 4.54329…\n$ Serum_Amyloid_P                  &lt;dbl&gt; -5.599422, -6.119298, -5.381699, -6.645391, -5.203007, -5…\n$ Sortilin                         &lt;dbl&gt; 4.908629, 5.478731, 3.810182, 3.402176, 3.402176, 2.97881…\n$ Stem_Cell_Factor                 &lt;dbl&gt; 4.174387, 3.713572, 3.433987, 3.951244, 4.060443, 2.56494…\n$ TGF_alpha                        &lt;dbl&gt; 8.649098, 11.331619, 10.858497, 9.454406, 8.323453, 10.00…\n$ TIMP_1                           &lt;dbl&gt; 15.204651, 11.266499, 12.282857, 11.114877, 13.748016, 11…\n$ TNF_RII                          &lt;dbl&gt; -0.06187540, -0.32850407, -0.41551544, -0.34249031, -0.34…\n$ TRAIL_R3                         &lt;dbl&gt; -0.1829004, -0.5007471, -0.9240345, -0.3848591, -0.858259…\n$ TTR_prealbumin                   &lt;dbl&gt; 2.944439, 2.833213, 2.944439, 2.944439, 3.044522, 3.04452…\n$ Tamm_Horsfall_Protein_THP        &lt;dbl&gt; -3.095810, -3.111190, -3.166721, -3.155652, -3.038017, -3…\n$ Thrombomodulin                   &lt;dbl&gt; -1.340566, -1.675252, -1.534276, -1.975407, -1.210709, -1…\n$ Thrombopoietin                   &lt;dbl&gt; -0.1026334, -0.6733501, -0.9229670, -0.7510004, 0.0976177…\n$ Thymus_Expressed_Chemokine_TECK  &lt;dbl&gt; 4.149327, 3.810182, 2.791992, 4.037285, 4.534163, 4.53416…\n$ Thyroid_Stimulating_Hormone      &lt;dbl&gt; -3.863233, -4.828314, -4.990833, -4.892852, -4.645992, -4…\n$ Thyroxine_Binding_Globulin       &lt;dbl&gt; -1.4271164, -1.6094379, -1.8971200, -2.0402208, -0.478035…\n$ Tissue_Factor                    &lt;dbl&gt; 2.04122033, 2.02814825, 1.43508453, 2.02814825, 1.9878743…\n$ Transferrin                      &lt;dbl&gt; 3.332205, 2.890372, 2.890372, 2.890372, 3.496508, 2.99573…\n$ Trefoil_Factor_3_TFF3            &lt;dbl&gt; -3.381395, -3.912023, -3.729701, -3.816713, -3.442019, -4…\n$ VCAM_1                           &lt;dbl&gt; 3.258097, 2.708050, 2.639057, 2.772589, 3.044522, 2.20827…\n$ VEGF                             &lt;dbl&gt; 22.03456, 18.60184, 17.47619, 17.54560, 20.77860, 13.1976…\n$ Vitronectin                      &lt;dbl&gt; -0.04082199, -0.38566248, -0.22314355, -0.65392647, 0.166…\n$ von_Willebrand_Factor            &lt;dbl&gt; -3.146555, -3.863233, -3.540459, -3.863233, -3.816713, -4…\n$ age                              &lt;dbl&gt; 0.9876238, 0.9861496, 0.9866667, 0.9867021, 0.9871630, 0.…\n$ tau                              &lt;dbl&gt; 6.297754, 6.659294, 6.270988, 6.152733, 6.623707, 5.36129…\n$ p_tau                            &lt;dbl&gt; 4.348108, 4.859967, 4.400247, 4.494886, 4.524589, 3.46573…\n$ Ab_42                            &lt;dbl&gt; 12.019678, 11.015759, 12.302271, 12.398138, 11.024109, 11…\n$ male                             &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, …\n$ Genotype                         &lt;fct&gt; E3E3, E3E4, E3E4, E3E4, E3E3, E4E4, E2E3, E2E3, E3E3, E2E…\n$ Class                            &lt;fct&gt; Control, Control, Control, Control, Control, Impaired, Co…"
  },
  {
    "objectID": "slides/03-oblique_forests.html#model-fitting",
    "href": "slides/03-oblique_forests.html#model-fitting",
    "title": "Oblique random forests with aorsf",
    "section": "Model fitting",
    "text": "Model fitting\n\nFormula interface\n\nSimilar to other R packages\nShortcut: outcome ~ .\n\n\nlibrary(aorsf)\nfit &lt;- orsf(ad_data, Class ~ .)\n\n\n\n\n\n---------- Oblique random classification forest\n\n     Linear combinations: Accelerated Logistic regression\n          N observations: 333\n               N classes: 2\n                 N trees: 500\n      N predictors total: 130\n   N predictors per node: 12\n Average leaves per tree: 13.122\nMin observations in leaf: 5\n          OOB stat value: 0.88\n           OOB stat type: AUC-ROC\n     Variable importance: anova\n\n-----------------------------------------"
  },
  {
    "objectID": "slides/03-oblique_forests.html#your-turn",
    "href": "slides/03-oblique_forests.html#your-turn",
    "title": "Oblique random forests with aorsf",
    "section": "Your turn",
    "text": "Your turn\nOpen classwork/03-oblique_forests.qmd and complete Exercise 1\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/03-oblique_forests.html#random-splits",
    "href": "slides/03-oblique_forests.html#random-splits",
    "title": "Oblique random forests with aorsf",
    "section": "Random splits",
    "text": "Random splits\nCompare ‘random splits’ to the accelerated regression approach.\n\nCreate random linear combinations\n\n\nf_rando &lt;- function(x_node, y_node, w_node){\n  matrix(runif(ncol(x_node)), ncol=1) \n}\n\ncontrol_rando &lt;- orsf_control_classification(f_rando)\n\n\nKeep making them until a split is found that is ‘good enough’\n\n\nfit_rando = orsf(ad_data, formula = Class ~ ., \n                 tree_seeds = 1, \n                 n_retry = 100,\n                 split_rule = 'cstat', \n                 split_min_stat = .6,\n                 control = control_rando)"
  },
  {
    "objectID": "slides/03-oblique_forests.html#compare-computational-efficiency",
    "href": "slides/03-oblique_forests.html#compare-computational-efficiency",
    "title": "Oblique random forests with aorsf",
    "section": "Compare computational efficiency",
    "text": "Compare computational efficiency\n‘Many splits’ is slower\n\nmicrobenchmark::microbenchmark(\n  rando = orsf(ad_data, formula = Class ~ ., \n               tree_seeds = 1, n_retry = 500,\n               split_rule = 'cstat', split_min_stat = .6,\n               control = control_rando),\n  accel = orsf(ad_data, formula = Class ~ ., \n               tree_seeds = 1, split_rule = 'cstat',\n               split_min_stat = .6),\n  times = 5\n)\n\nUnit: milliseconds\n  expr       min        lq      mean   median       uq       max neval cld\n rando 1103.7639 1111.5100 1173.4211 1133.069 1135.577 1383.1855     5  a \n accel  101.4826  109.4326  115.1902  115.645  119.620  129.7707     5   b"
  },
  {
    "objectID": "slides/03-oblique_forests.html#compare-out-of-bag-prediction-accuracy",
    "href": "slides/03-oblique_forests.html#compare-out-of-bag-prediction-accuracy",
    "title": "Oblique random forests with aorsf",
    "section": "Compare out-of-bag prediction accuracy",
    "text": "Compare out-of-bag prediction accuracy\n\nOut-of-bag C-statistic for random splits:\n\n\nfit_rando$eval_oobag$stat_values\n\n          [,1]\n[1,] 0.7875307\n\n\n\nOut-of-bag C-statistic for accelerated regression:\n\n\nfit_accel$eval_oobag$stat_values\n\n          [,1]\n[1,] 0.8968304\n\n\n\\(\\Rightarrow\\) accelerated regression is faster and more accurate"
  },
  {
    "objectID": "slides/03-oblique_forests.html#partial-dependence",
    "href": "slides/03-oblique_forests.html#partial-dependence",
    "title": "Oblique random forests with aorsf",
    "section": "Partial dependence",
    "text": "Partial dependence\nPurpose: Interpret a model.\nDescription: Expected prediction from a model as a function of a single predictor or multiple predictors. The expectation is marginalized over the values of all other predictors, giving something like a multivariable adjusted estimate of the model’s prediction.\nExample: Expected 10-year mortality risk as a function of age.\nPseudo-code:\n\nfor i in seq(min_age, max_age){\n  \n - Set all values of age to i in the training data\n - Compute out-of-bag predictions for training data\n - Save the mean prediction and i\n  \n}"
  },
  {
    "objectID": "slides/03-oblique_forests.html#single-variable-summary",
    "href": "slides/03-oblique_forests.html#single-variable-summary",
    "title": "Oblique random forests with aorsf",
    "section": "Single variable summary",
    "text": "Single variable summary\nSimilar to summary(), orsf_summarize_uni() computes out-of-bag partial dependence on the number of variables you request.\n\norsf_summarize_uni(fit, n_variables = 1, class = \"Impaired\")\n\n\n-- tau (VI Rank: 1) ----------------------------\n\n        |------------ Probability ------------|\n  Value      Mean    Median    25th %    75th %\n &lt;char&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n   5.04 0.2322465 0.1929924 0.1187406 0.3173001\n   5.37 0.2369582 0.1963388 0.1223932 0.3234504\n   5.75 0.2529760 0.2109251 0.1293360 0.3498093\n   6.18 0.2817313 0.2352786 0.1531655 0.3805347\n   6.50 0.3011033 0.2557926 0.1721181 0.4076272\n\n Predicted probability for top 1 predictors"
  },
  {
    "objectID": "slides/03-oblique_forests.html#multi-variable-summary",
    "href": "slides/03-oblique_forests.html#multi-variable-summary",
    "title": "Oblique random forests with aorsf",
    "section": "Multi-variable summary",
    "text": "Multi-variable summary\nSee if predictors interact (they don’t in this case)\n\npd &lt;- orsf_pd_oob(fit, pred_spec_auto(tau, Genotype))"
  },
  {
    "objectID": "slides/03-oblique_forests.html#your-turn-1",
    "href": "slides/03-oblique_forests.html#your-turn-1",
    "title": "Oblique random forests with aorsf",
    "section": "Your turn",
    "text": "Your turn\nComplete exercise 2\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/03-oblique_forests.html#variable-selection",
    "href": "slides/03-oblique_forests.html#variable-selection",
    "title": "Oblique random forests with aorsf",
    "section": "Variable selection",
    "text": "Variable selection\nWe want to remove predictors that don’t have any prognostic value.\n\norsf_vs() performs recursive feature elimination.\nFit a forest and estimate importance\nDrop the least important predictor\nRepeat until &lt; n_predictor_min predictors left.\n\n\norsf_vars &lt;- orsf_vs(fit, \n                     n_predictor_min = 1, \n                     verbose_progress = TRUE)\n\nSelecting variables: 34.9% ~ time remaining: 6 seconds \nSelecting variables: 81.4% ~ time remaining: 1 seconds \nSelecting variables: 100%"
  },
  {
    "objectID": "slides/03-oblique_forests.html#faster",
    "href": "slides/03-oblique_forests.html#faster",
    "title": "Oblique random forests with aorsf",
    "section": "Faster",
    "text": "Faster\nIf needed, reduce n_tree and increase leaf_min_obs to speed this up\n\nfit_light &lt;- orsf_update(fit, n_tree = 100, leaf_min_obs = 20)\n\norsf_vars_light &lt;- orsf_vs(fit_light, n_predictor_min = 1, verbose_progress = TRUE)\n\nSelecting variables: 100%"
  },
  {
    "objectID": "slides/03-oblique_forests.html#pick-your-variables",
    "href": "slides/03-oblique_forests.html#pick-your-variables",
    "title": "Oblique random forests with aorsf",
    "section": "Pick your variables",
    "text": "Pick your variables\nPlot the history of performance and variables included.\n\n\nmax_auc &lt;- orsf_vars %&gt;% \n  arrange(desc(stat_value)) %&gt;% \n  slice(1)\n\nfig &lt;- ggplot(orsf_vars) + \n  aes(x = n_predictors,\n      y = stat_value) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(se = FALSE, \n              linewidth = 1) + \n  geom_point(data = max_auc, \n             size = 3, \n             color = 'purple')"
  },
  {
    "objectID": "slides/03-oblique_forests.html#your-turn-2",
    "href": "slides/03-oblique_forests.html#your-turn-2",
    "title": "Oblique random forests with aorsf",
    "section": "Your turn",
    "text": "Your turn\nComplete exercise 3\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/03-oblique_forests.html#compare-models",
    "href": "slides/03-oblique_forests.html#compare-models",
    "title": "Oblique random forests with aorsf",
    "section": "Compare models",
    "text": "Compare models\nFor causal random forests, we’ll need good prediction models for both dementia and APOE4 status.\n\nDeveloping and evaluating prediction models is a deep topic.\nWe will cover the basics:\n\nData splitting\nPre-processing\nEvaluating in test data\n\n\nWe’ll do this using the tidymodels framework."
  },
  {
    "objectID": "slides/03-oblique_forests.html#data-splitting",
    "href": "slides/03-oblique_forests.html#data-splitting",
    "title": "Oblique random forests with aorsf",
    "section": "Data splitting",
    "text": "Data splitting\nIf you evaluate prediction models with their training data, you\n\nReward models that overfit the data.\nDon’t learn much about what will happen in the real world.\n\nSo, always evaluate prediction models with data that is “new”\n\nlibrary(rsample)\nset.seed(1234)\n# make the split: 50% training data, 50% testing\nsplit &lt;- initial_split(data = ad_data, prop = 1/2)\n# get the training data\ntrain &lt;- training(split)\n# get the testing data\ntest &lt;- testing(split)"
  },
  {
    "objectID": "slides/03-oblique_forests.html#pre-processing",
    "href": "slides/03-oblique_forests.html#pre-processing",
    "title": "Oblique random forests with aorsf",
    "section": "Pre-processing",
    "text": "Pre-processing\nWhy do we pre-process data?\n\nSome models require special data format\nImpute missing values correctly\nSometimes its helpful to apply transformations to data prior to modeling\n\nWe’ll train our pre-processing steps using the recipes package\n\nlibrary(recipes)\n\n# Makes the recipe steps\nrecipe_steps &lt;- recipe(data = train, Class ~ .) %&gt;% \n  step_impute_knn(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors())"
  },
  {
    "objectID": "slides/03-oblique_forests.html#training-the-recipe",
    "href": "slides/03-oblique_forests.html#training-the-recipe",
    "title": "Oblique random forests with aorsf",
    "section": "Training the recipe",
    "text": "Training the recipe\nWith its steps defined, we are ready to\n\ntrain the recipe\n\n\n# train the recipe steps with training data\nrecipe_trained &lt;- prep(recipe_steps, training = train)\n\n\nuse it to process the training and testing data.\n\n\n# apply the recipe steps to process training data\ntrain_processed &lt;- bake(recipe_trained, new_data = train)\n# apply the exact same steps to the testing data\ntest_processed &lt;- bake(recipe_trained, new_data = test)"
  },
  {
    "objectID": "slides/03-oblique_forests.html#fitting-models",
    "href": "slides/03-oblique_forests.html#fitting-models",
    "title": "Oblique random forests with aorsf",
    "section": "Fitting models",
    "text": "Fitting models\n\nparsnip is a unified interface for models with R.\nbonsai extends parsnip, focusing on decision tree models.\n\nWith these packages, we make a model specification, and then fit it.\n\nlibrary(parsnip)\nlibrary(bonsai)\n\n# make model specifications\nranger_spec &lt;- rand_forest(mode = 'classification', engine = 'ranger')\naorsf_spec  &lt;- rand_forest(mode = 'classification', engine = 'aorsf')\n\n# fit them\nranger_fit &lt;- fit(ranger_spec, data = train_processed, formula = Class ~ .)\naorsf_fit  &lt;- fit(aorsf_spec,  data = train_processed, formula = Class ~ .)"
  },
  {
    "objectID": "slides/03-oblique_forests.html#evaluating-in-test-data",
    "href": "slides/03-oblique_forests.html#evaluating-in-test-data",
    "title": "Oblique random forests with aorsf",
    "section": "Evaluating in test data",
    "text": "Evaluating in test data\nWe predict probabilities for observations in the testing data, and evaluate discrimination using the yardstick package:\n\npreds &lt;- list(aorsf = aorsf_fit, ranger = ranger_fit) %&gt;% \n  map(~ predict(.x, new_data = test_processed, type = 'prob')) %&gt;% \n  map(~ mutate(.x, truth = test_processed$Class)) %&gt;% \n  bind_rows(.id = 'model')\n\nlibrary(yardstick)\n\npreds %&gt;% \n  group_by(model) %&gt;% \n  roc_auc(truth = truth, .pred_Impaired)\n\n\n\n# A tibble: 3 × 4\n  model  .metric .estimator .estimate\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 aorsf  roc_auc binary         0.936\n2 odrf   roc_auc binary         0.940\n3 ranger roc_auc binary         0.918"
  },
  {
    "objectID": "slides/03-oblique_forests.html#to-the-pipeline",
    "href": "slides/03-oblique_forests.html#to-the-pipeline",
    "title": "Oblique random forests with aorsf",
    "section": "To the pipeline",
    "text": "To the pipeline\n\nCopy/paste this code into your _targets.R file.\n\n\n# in _targets.R, you should see this comment: \n\n# real data model targets (to be added as an exercise). \n\n# Paste this code right beneath that.\n\nfit_aorsf_zzzz_tar &lt;- tar_target(\n  fit_aorsf_zzzz,\n  fit_orsf_clsf(data = data_zzzz)\n)\n\n\nModify this code, replacing zzzz with the name of your dataset."
  },
  {
    "objectID": "slides/03-oblique_forests.html#references",
    "href": "slides/03-oblique_forests.html#references",
    "title": "Oblique random forests with aorsf",
    "section": "References",
    "text": "References\n\n\nSlides available at https://bcjaeger.github.io/melodem-apoe4-het/\n\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nJaeger, Byron C, Sawyer Welden, Kristin Lenoir, and Nicholas M Pajewski. 2022. “Aorsf: An r Package for Supervised Learning Using the Oblique Random Survival Forest.” Journal of Open Source Software 7 (77): 4705.\n\n\nJaeger, Byron C, Sawyer Welden, Kristin Lenoir, Jaime L Speiser, Matthew W Segar, Ambarish Pandey, and Nicholas M Pajewski. 2024. “Accelerated and Interpretable Oblique Random Survival Forests.” Journal of Computational and Graphical Statistics 33 (1): 192–207.\n\n\nKatuwal, Rakesh, Ponnuthurai Nagaratnam Suganthan, and Le Zhang. 2020. “Heterogeneous Oblique Random Forest.” Pattern Recognition 99: 107078.\n\n\nMenze, Bjoern H, B Michael Kelm, Daniel N Splitthoff, Ullrich Koethe, and Fred A Hamprecht. 2011. “On Oblique Random Forests.” In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2011, Athens, Greece, September 5-9, 2011, Proceedings, Part II 22, 453–69. Springer."
  },
  {
    "objectID": "slides/04-causal_forests.html#references",
    "href": "slides/04-causal_forests.html#references",
    "title": "Causal random forests with grf",
    "section": "References",
    "text": "References\n\n\nSlides available at https://bcjaeger.github.io/melodem-apoe4-het/\n\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/debiased machine learning for treatment and structural parameters.” The Econometrics Journal 21 (1): C1–68. https://doi.org/10.1111/ectj.12097.\n\n\nCui, Yifan, Michael R Kosorok, Erik Sverdrup, Stefan Wager, and Ruoqing Zhu. 2023. “Estimating Heterogeneous Treatment Effects with Right-Censored Data via Causal Survival Forests.” Journal of the Royal Statistical Society Series B: Statistical Methodology 85 (2): 179–211.\n\n\nRobinson, Peter M. 1988. “Root-n-Consistent Semiparametric Regression.” Econometrica: Journal of the Econometric Society, 931–54.\n\n\nSemenova, Vira, and Victor Chernozhukov. 2021. “Debiased Machine Learning of Conditional Average Treatment Effects and Other Causal Functions.” The Econometrics Journal 24 (2): 264–89.\n\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42.\n\n\nYadlowsky, Steve, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. 2021. “Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects.” arXiv Preprint arXiv:2111.07966."
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-robinsons-regression",
    "href": "slides/04-causal_forests.html#intuition-of-robinsons-regression",
    "title": "Causal random forests with grf",
    "section": "Intuition of Robinson’s regression",
    "text": "Intuition of Robinson’s regression\n\n\n\\(Y - E[Y|X]\\) is a treatment centered Y.\n\\(W - E[W|X]\\) is the usual binary treatment effect in randomized trials"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-robinsons-regression-1",
    "href": "slides/04-causal_forests.html#intuition-of-robinsons-regression-1",
    "title": "Causal random forests with grf",
    "section": "Intuition of Robinson’s regression",
    "text": "Intuition of Robinson’s regression\n\n\n\\(W - E[W|X]\\) is a continuous exposure for the treatment in observational data\nCentered \\(W\\) impacts how much a person with covariates \\(X\\) influences the \\(\\tau\\) estimate"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-robinsons-regression-2",
    "href": "slides/04-causal_forests.html#intuition-of-robinsons-regression-2",
    "title": "Causal random forests with grf",
    "section": "Intuition of Robinson’s regression",
    "text": "Intuition of Robinson’s regression\n\n\n\\(Y - E[Y|X]\\) is a treatment centered Y.\n\\(W - E[W|X]\\) is a nuisance parameter. The conditional mean of \\(Y\\) is what matters most for \\(\\tau\\)"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-causal-trees",
    "href": "slides/04-causal_forests.html#intuition-of-causal-trees",
    "title": "Causal random forests with grf",
    "section": "Intuition of causal trees",
    "text": "Intuition of causal trees\n\n\nEveryone starts at the root"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-causal-trees-1",
    "href": "slides/04-causal_forests.html#intuition-of-causal-trees-1",
    "title": "Causal random forests with grf",
    "section": "Intuition of causal trees",
    "text": "Intuition of causal trees\n\n\nAssess splits by running Robinson’s regression in each child node.\nRemember: good splits maximize \\[n_L \\cdot n_R \\cdot (\\hat{\\tau}_L-\\hat{\\tau}_R)^2\\]\nIs this a good split?"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-causal-trees-2",
    "href": "slides/04-causal_forests.html#intuition-of-causal-trees-2",
    "title": "Causal random forests with grf",
    "section": "Intuition of causal trees",
    "text": "Intuition of causal trees\n\n\nAssess splits by running Robinson’s regression in each child node.\nRemember: good splits maximize \\[n_L \\cdot n_R \\cdot (\\hat{\\tau}_L-\\hat{\\tau}_R)^2\\]\nIs this a good split?"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-forest-weights",
    "href": "slides/04-causal_forests.html#intuition-of-forest-weights",
    "title": "Causal random forests with grf",
    "section": "Intuition of forest weights",
    "text": "Intuition of forest weights\n\n\nStart with a “new” data point: Bill\nInitialize leaf counts and no. of trees as 0."
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-forest-weights-1",
    "href": "slides/04-causal_forests.html#intuition-of-forest-weights-1",
    "title": "Causal random forests with grf",
    "section": "Intuition of forest weights",
    "text": "Intuition of forest weights\n\n\nDrop Bill down the first tree"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-forest-weights-2",
    "href": "slides/04-causal_forests.html#intuition-of-forest-weights-2",
    "title": "Causal random forests with grf",
    "section": "Intuition of forest weights",
    "text": "Intuition of forest weights\n\n\nDrop Bill down the first tree\nUpdate counts"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-forest-weights-3",
    "href": "slides/04-causal_forests.html#intuition-of-forest-weights-3",
    "title": "Causal random forests with grf",
    "section": "Intuition of forest weights",
    "text": "Intuition of forest weights\n\n\nDrop Bill down the second tree\nUpdate counts"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-forest-weights-4",
    "href": "slides/04-causal_forests.html#intuition-of-forest-weights-4",
    "title": "Causal random forests with grf",
    "section": "Intuition of forest weights",
    "text": "Intuition of forest weights\n\n\nCompute weights\nNote some are 0"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-forest-weights-5",
    "href": "slides/04-causal_forests.html#intuition-of-forest-weights-5",
    "title": "Causal random forests with grf",
    "section": "Intuition of forest weights",
    "text": "Intuition of forest weights\n\n\nThese are Bill’s “neighbors”\nSome are more neighbor than others"
  },
  {
    "objectID": "slides/04-causal_forests.html#intuition-of-forest-weights-6",
    "href": "slides/04-causal_forests.html#intuition-of-forest-weights-6",
    "title": "Causal random forests with grf",
    "section": "Intuition of forest weights",
    "text": "Intuition of forest weights\n\n\nNow run Robinson’s regression with these weights\nBill gets his own \\(\\hat\\tau\\) from this."
  }
]