---
title: "Causal random forests with `grf`"
subtitle: "MELODEM data workshop"
author: "Byron C. Jaeger, PhD"
institute: "Wake Forest University School of Medicine"
format: 
  revealjs:
    slide-number: true
    footer: Slides available at <https://bcjaeger.github.io/melodem-apoe4-het/>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
---

```{r setup, cache=FALSE, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE,
                      warning = FALSE,
                      dpi = 300,
                      cache = FALSE,
                      fig.height = 7.25,
                      out.width = '100%',
                      fig.align = 'center')


withr::with_dir(
  new = here::here(), code = {
    R.utils::sourceDirectory('R')
    library(tidyverse)
    library(tidymodels)
    library(butcher)
    library(grf)
    targets::tar_load(data_melodem)
  }
)

```

## Overview

- Robinson's residual-on-residual regression

    + Causal trees
    
    + Causal random forest
    
    + Causal random survival forest

- Inference with causal random forests

    + Conditional average treatment effect (CATE)
    
    + Best linear projection (BLP)
    
    + Rank weighted average treatment effect (RATE)

# Robinson's residual-on-residual regression

## The partially linear model

Suppose

$$
Y_i = \tau W_i + f(X_i) + \varepsilon_i
$$
Assume: 

- $E[\varepsilon_i | X_i, W_i] = 0$

- untreated outcome is given by unknown function $f$, 

- a treatment assignment shifts the outcome by $\tau$.


## How to estimate $\tau$?

Suppose

$$
Y_i = \tau W_i + f(X_i) + \varepsilon_i 
$$

How do we estimate $\tau$ when we do not know $f(X_i)$? 

Define:

\begin{align*}

e(x) &= E[W_i | X_i=x] \,\, \text{(Propensity score)} \\

m(x) &= E[Y_i | X_i = x] = f(x) + \tau e(x) \,\,\,\,\, \text{(Cndl. mean of } Y\text{)}

\end{align*}

## Use propensity and conditional mean

Re-express the partial linear model in terms of $e(x)$ and $m(x)$:

\begin{align*}

Y_i &= \tau W_i + f(X_i) + \varepsilon_i, \,  \\

Y_i - \tau e (x) &= \tau W_i + f(X_i) - \tau e(x) + \varepsilon_i, \, \\

Y_i - f(X_i) - \tau e (x) &= \tau W_i - \tau e (x) + \varepsilon_i, \, \\

Y_i - m(x) &= \tau (W_i - e(x)) + \varepsilon_i, \, \\

\end{align*}

$\tau$ can be estimated with residual-on-residual regression [@robinson1988root]. 

**How?** Plug in flexible estimates of $m(x)$ and $e(x)$

## Re-write as a linear model

More formally, 

$$
\hat{\tau} := \text{lm}\Biggl( Y_i - \hat m^{(-i)}(X_i) \sim W_i - \hat e^{(-i)}(X_i)\Biggr).
$$

- Superscript $^i$ denotes cross-fit estimates [@debiased_ML_Chernozhukov]. 

- Cross-fitting: estimate something, e.g., $e(x)$, using cross-validation. 

- Why? removes bias from over-fitting.

## Example

:::{.column width="45%"}

Suppose $Y_i = \tau W_i + f(X_i) + \epsilon_i$

- $\tau$ is 1/2
- $W_i$ is randomized treatment
- $X_i$ is a continuous covariate
- $f(X_i) = |X_i|$

By defn, $E[W_i] = 1/2$ (why?). 

:::

:::{.column width="53%"}

```{r echo=TRUE}

# Set up for const tau example

set.seed(1)
tau_truth <- 1/2
n <- 1000

# randomized treatment
W <- rbinom(n = n, size = 1, prob = 1/2)

# continuous covariate
X <- rnorm(n = n)

# outcome 
Y <- tau_truth * W + abs(X) + rnorm(n)

data <- data.frame(Y=Y, X=X, W=W)

```

:::

## Example done wrong

:::{.column width="45%"}

First we'll do it the wrong way.

- Fit a **classical** model to estimate conditional mean of $Y$.

- Compute residuals and run Robinson's regression.

- What'd we do wrong?

:::

:::{.column width="53%"}

```{r, echo=TRUE}

library(glue)
library(ggplot2)

fit_cmean <- lm(Y ~ X, data = data)

m_x <- predict(fit_cmean, new_data = data)

resid_y <- Y - m_x
resid_w <- W - 1/2

tau_fit <- lm(resid_y ~ resid_w) 

glue("True tau is {tau_truth}, \\
      estimated tau is {coef(tau_fit)[2]}")


```

:::

## Conditional mean predictions...

```{r, fig.width = 6, fig.height = 6}

mse <- round(mean((m_x - Y)^2), 2)

ggplot(data = data.frame(predicted = m_x, observed = Y)) + 
  aes(x = predicted, y = observed) + 
  geom_point() + 
  geom_smooth() + 
  labs(title = glue("Linear model predictions: MSE = {mse}"))


```

## Example done wrong, take 2

:::{.column width="45%"}

The model for conditional mean was under-specified.

- Fit a **flexible** model to estimate conditional mean of $Y$.

- Compute residuals and run Robinson's regression.

- What'd we do wrong?

:::

:::{.column width="53%"}

```{r, echo=TRUE}

library(aorsf)

fit_cmean <- orsf(Y ~ X, data = data)

m_x <- predict(fit_cmean, new_data = data)

resid_y <- Y - m_x
resid_w <- W - 1/2

tau_fit <- lm(resid_y ~ resid_w) 

glue("True tau is {tau_truth}, \\
      estimated tau is {coef(tau_fit)[2]}")


```

:::

## Example done right

:::{.column width="45%"}

We forgot about cross-fitting!

- Fit a flexible model to estimate conditional mean of $Y$.

- Use **out-of-bag predictions**.

- Compute residuals and run Robinson's regression.

:::

:::{.column width="53%"}

```{r, echo=TRUE}

m_x_oobag <- predict(fit_cmean, oobag = TRUE)

resid_y <- Y - m_x_oobag
resid_w <- W - 1/2

tau_fit <- lm(resid_y ~ resid_w) 

glue("True tau is {tau_truth}, \\
      estimated tau is {coef(tau_fit)[2]}")


```

:::

## Conditional mean predictions

```{r, fig.width=12, fig.height=6}

p1 <- ggplot(data = data.frame(predicted = m_x, 
                               observed = Y)) + 
  aes(x = predicted, y = observed) + 
  geom_point() + 
  geom_smooth()

p2 <- ggplot(data = data.frame(predicted = m_x_oobag, 
                               observed = Y)) + 
  aes(x = predicted, y = observed) + 
  geom_point() + 
  geom_smooth()

library(table.glue)

library(patchwork)

(p1 + labs(title=table_glue("In-bag predictions: MSE = {mean((m_x - Y)^2)}"))) + 
  (p2 + labs(title=table_glue("Out-of-bag predictions: MSE = {mean((m_x_oobag - Y)^2)}"))) + 
  plot_layout(axes = 'collect')

```

## Intuition of Robinson's regression

:::{.column width="39%"}

- $Y - E[Y|X]$ is a treatment centered Y.

- $W - E[W|X]$ is the usual binary treatment effect in randomized trials

:::

:::{.column width="60%"}

![](diagram/robin-regress-1.svg){width=7in}
:::

## Intuition of Robinson's regression

:::{.column width="39%"}

- $W - E[W|X]$ is a continuous exposure for the treatment in observational data

- Centered $W$ impacts how much a person with covariates $X$ influences the $\tau$ estimate

:::

:::{.column width="60%"}

![](diagram/robin-regress-2.svg){width=7in}
:::


# Causal trees

## How to grow causal trees

Causal trees are much like standard decision trees, but they maximize

$$n_L \cdot n_R \cdot (\hat{\tau}_L-\hat{\tau}_R)^2$$

where residual-on-residual regression is used to estimate $\hat{\tau}_L$ and $\hat{\tau}_R$

- `grf` estimates $\hat \tau$ *once* in the parent node and uses "influence functions" to approximate how $\hat\tau$ would change if an observation moved from one child node to the other [@wager2018estimation]. 

- Predictions from leaves are $E[Y|W=1] - E[Y|W=0]$


## How to grow causal trees

Causal trees use "honesty" and "subsampling" [@wager2018estimation]. 

- **Honesty**: Each training observation is used for **one** of the following:

    + Estimate the treatment effect for leaf nodes.

    + Decide splitting values for non-leaf nodes.

- **Subsampling**: While @breiman2001random's random forest uses bootstrap sampling with replacement, the causal random forest samples without replacement. 

## Intuition of causal trees

:::{.column width="39%"}

- Everyone starts at the root

:::

:::{.column width="60%"}

![](diagram/causal-tree-1.svg){width=7in}
:::

## Intuition of causal trees

:::{.column width="39%"}

- Assess splits by running Robinson's regression in each child node.

- Remember: good splits maximize $$n_L \cdot n_R \cdot (\hat{\tau}_L-\hat{\tau}_R)^2$$

- Is this a good split?


:::

:::{.column width="60%"}

![](diagram/causal-tree-2.svg){width=7in}
:::

## Intuition of causal trees

:::{.column width="39%"}

- Assess splits by running Robinson's regression in each child node.

- Remember: good splits maximize $$n_L \cdot n_R \cdot (\hat{\tau}_L-\hat{\tau}_R)^2$$

- Is this a good split?

:::

:::{.column width="60%"}

![](diagram/causal-tree-3.svg){width=7in}
:::

# Causal random forest

## Back to the partial linear model

Relaxing the assumption of a constant treatment:

$$
Y_i = \color{red}{\tau(X_i)} W_i + f(X_i) + \varepsilon_i, \, 
$$

where $\color{red}{\tau(X_i)}$ is the conditional average treatment (CATE). If we had a neighborhood $\mathcal{N}(x)$ where $\tau$ was constant, then we could do residual-on-residual regression in the neighborhood:

$$
\hat\tau_i(x) := lm\Biggl( Y_i - \hat m^{(-i)}(X_i) \sim W_i - \hat e^{(-i)}(X_i), \color{red}{w = 1\{X_i \in \mathcal{N}(x) \}}\Biggr),
$$

## Random forest adaptive neighborhoods

Suppose we fit a random forest with $B$ trees to a training set of size $n$, and we compute a prediction $p$ for a new observation $x$:

\begin{equation*}
\begin{split}
& p = \sum_{i=1}^{n} \frac{1}{B} \sum_{b=1}^{B} Y_i \frac{1\{Xi \in L_b(x)\}} {|L_b(x)|}
\end{split}
\end{equation*}

- $L_b(x)$ indicates the leaf node that $x$ falls into for tree $b$

- The inner sum is the mean of outcomes in the same leaf as $x$

- This generalizes to causal random forests (it's easier to write with regression trees).

## Random forest adaptive neighborhoods

Pull $Y_i$ out of the sum that depends on $b$:

\begin{equation*}
\begin{split}
p &= \sum_{i=1}^{n} \frac{1}{B} \sum_{b=1}^{B} Y_i \frac{1\{Xi \in L_b(x)\}} {|L_b(x)|} \\
&= \sum_{i=1}^{n} Y_i \sum_{b=1}^{B} \frac{1\{Xi \in L_b(x)\}} {B \cdot |L_b(x)|} \\
& = \sum_{i=1}^{n} Y_i \color{blue}{\alpha_i(x)},
\end{split}
\end{equation*}

- $\alpha_i(x) \propto$ no. of times observation $i$ lands in the same leaf as $x$

## Intuition of forest weights

:::{.column width="39%"}

- Start with a "new" data point: Bill

- Initialize leaf counts and no. of trees as 0.

:::

:::{.column width="60%"}

![](diagram/causal-tree-weights-1.svg){width=7in}
:::

## Intuition of forest weights

:::{.column width="39%"}
- Drop Bill down the first tree
:::

:::{.column width="60%"}
![](diagram/causal-tree-weights-2.svg){width=7in}
:::

## Intuition of forest weights

:::{.column width="39%"}
- Drop Bill down the first tree
- Update counts
:::

:::{.column width="60%"}
![](diagram/causal-tree-weights-3.svg){width=7in}
:::

## Intuition of forest weights

:::{.column width="39%"}
- Drop Bill down the second tree
- Update counts
:::

:::{.column width="60%"}
![](diagram/causal-tree-weights-4.svg){width=7in}
:::

## Intuition of forest weights

:::{.column width="39%"}
- Compute weights
- Note some are 0
:::

:::{.column width="60%"}
![](diagram/causal-tree-weights-5.svg){width=7in}
:::

## Intuition of forest weights

:::{.column width="39%"}
- These are Bill's "neighbors"
- Some are more neighbor than others
:::

:::{.column width="60%"}
![](diagram/causal-tree-weights-6.svg){width=7in}
:::

## Intuition of forest weights

:::{.column width="39%"}
- Now run Robinson's regression with these weights
- Bill gets his own $\hat\tau$ from this.
:::

:::{.column width="60%"}
![](diagram/causal-tree-weights-7.svg){width=7in}
:::


## Plug weights in to `lm`

Instead of defining neighborhood boundaries, weight by similarity:

$$
\hat\tau_i(x) := \text{lm}\Biggl( Y_i - \hat m^{(-i)}(X_i) \sim W_i - \hat e^{(-i)}(X_i), w = \color{blue}{\alpha_i(x)} \Biggr).
$$
This forest-localized version of Robinson's regression, paired with honesty and subsampling, gives asymptotic guarantees for estimation and inference [@wager2018estimation]:

1. Pointwise consistency for the true treatment effect.

2. Asymptotically Gaussian and centered sampling distribution.


## Three main components 

The procedure to estimate $\hat\tau_i$ has three pieces:

$$
\hat\tau_i(x) := \text{lm}\Biggl( Y_i - \color{green}{\hat m^{(-i)}(X_i)} \sim W_i - \color{red}{\hat e^{(-i)}(X_i)}, w = \color{blue}{\alpha_i(x)} \Biggr).
$$

1. $\color{green}{\hat m^{(-i)}(X_i)}$ is a flexible, cross-fit estimate for $E[Y|X]$

2. $\color{red}{\hat e^{(-i)}(X_i)}$ is a flexible, cross-fit estimate for $E[W|X]$

3. $\color{blue}{\alpha_i(x)}$ are the similarity weights from a causal random forest

# Causal random survival forest

## Set up

Assume the survival setting:

\begin{equation}
  Y_i =
    \begin{cases}
      T_i & \text{if } \, T_i \leq C_i \\
      C_i & \text{otherwise}
    \end{cases}
\end{equation}

Where $T_i$ is time to event and ($C_i$) is time to censoring. Define

\begin{equation}
D_i =
    \begin{cases}
      1 & \text{if } \, T_i \leq C_i \\
      0 & \text{otherwise.}
    \end{cases}
\end{equation}

## Observed time versus true time

:::{.column width="39%"}

Event times are obscured by

- censoring

- end of follow-up, i.e., $h$

:::

:::{.column width="60%"}

```{r fig.width=4, fig.height=4, out.width='80%'}

n <- 1e6
failure.time <- rexp(n, rate = 0.1)
censor.time <- runif(n, 0, 12)
Y <- pmin(failure.time, censor.time)
D <- as.integer(failure.time <= censor.time)
cc <- (D==1)

dens.cens <- with(density(failure.time), data.frame(x, y))
dens.obs <- with(density(Y[cc]), data.frame(x, y))
df <- rbind(dens.cens, dens.obs)

gg_data <- df

gg_data$distribution <-
  c(rep("Time to event", 
        nrow(dens.cens)), 
    rep("Observed time", 
        nrow(dens.obs)))

gg_data$distribution <-
  factor(gg_data$distribution,
         levels = c("Time to event", "Observed time"))

ggplot(gg_data) +
  aes(x = x,
      y = y,
      ymin = 0,
      ymax = y,
      fill = distribution) + 
  geom_line() +
  geom_ribbon(alpha = 0.5) +
  geom_vline(xintercept = 10) +
  xlab("Time") +
  ylab("Density") +
  labs(fill = "Distribution") +
  xlim(c(0, 30)) +
  scale_fill_manual(values = c("#F8766D", "#00BFC4")) +
  theme_classic() +
  theme(legend.position = 'inside',
        legend.position.inside = c(.8, .8))
```

:::

## Observed time versus true time

:::{.column width="39%"}

Event times are obscured by

- censoring

- end of follow-up, i.e., $h$

Estimate restricted mean survival time (RMST): $E \left[ \text{min}(T, h) \right]$. See @cui2023estimating for more details on adjustment for censoring.

:::

:::{.column width="60%"}

```{r fig.width=4, fig.height=4, out.width='80%'}

dens.obs <- subset(dens.obs, x <= 10)
gg_data <- rbind(dens.cens, dens.obs)

gg_data$distribution <- c(rep("Time to event", nrow(dens.cens)), rep("Observed time", nrow(dens.obs)))
gg_data$distribution[gg_data$x >= 10] <- "Truncate"
gg_data$distribution <- factor(gg_data$distribution, levels = c("Time to event", "Observed time", "Truncate"))

ggplot(gg_data) +
  aes(x = x, 
      y = y, 
      ymin = 0, 
      ymax = y, 
      fill = distribution,
      alpha = distribution) + 
  geom_line() +
  geom_ribbon(alpha = 0.5) +
  geom_vline(xintercept = 10) +
  xlab("Time") +
  ylab("Density") +
  labs(fill = "Distribution") +
  xlim(c(0, 30)) +
  scale_fill_manual(values = c("#F8766D", "#00BFC4", "#818181")) +
  scale_alpha_discrete(range = c(0.5, 1, 0.5)) +
  annotate(geom = 'text', x = 13, y = 0.05, color = 'black', hjust = -0.1,
           label = 'treat as observed\ncensor at h=10') +
  annotate("segment", x = 13, xend = 10, y = 0.05, yend = 0.05,
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  theme_classic() +
  guides(alpha = 'none') +
  theme(legend.position = 'inside',
        legend.position.inside = c(.8, .8))

```

:::

## Treatment effects for survival

Two treatment effects can be estimated conditional on $h$.

- RMST
$$\tau(x) = E[\min(T(1), h) - \min(T(0), h) \, | X = x],$$
- Survival probability:
$$\tau(x) = P[T(1) > h \, | X = x] - P[T(0) > h \, | X = x].$$
$T(1)$ and $T(0)$ are treated and untreated event times, respectively.

# Inference with causal random forests

## Summaries of CATEs

You could compute average treatment effect (ATE) as the mean of CATEs:

$$\hat\tau = \frac{1}{n}\sum_{i=1}^n \hat\tau_i(x)$$
But the augmented inverse probability weighted ATE is better:

$$
\hat \tau_{AIPW} = \frac{1}{n} \sum_{i=1}^{n}\left( \overbrace{\tau(X_i)}^{\text{Initial estimate}} + \overbrace{\frac{W_i - e(X_i)}{e(X_i)[1 - e(X_i)]}}^{\text{debiasing weight}} \cdot \overbrace{\left(Y_i - \mu(X_i, W_i)\right)}^{\text{residual}} \right)
$$

## Summaries of CATEs contd.

For simplicity, re-write the augmented inverse probability ATE as

$$\hat\tau = \frac{1}{n}\sum_{i=1}^n \hat\Gamma_i(x),$$
With a vector of these $\Gamma_i$'s, define:


- Average treatment effect (ATE) = `mean(gamma)`

- Best linear projection (BLP) = `lm(gamma ~ X)`

## Estimating the ATE

First, we'll prepare the data:

```{r, eval=FALSE, echo=TRUE}

# loads packages and R functions
targets::tar_load_globals()

# loads a specific target
tar_load(data_melodem)
```

Second, coerce data to `grf` format: 

```{r, echo = TRUE}

# helper function for grf data prep 
data_grf <- data_coerce_grf(data_melodem$values)

# just a view of the X matrix
head(data_grf$X, n = 2)

```

## Fitting the forest

Third, fit the causal survival forest: 

```{r, echo = TRUE}

fit_grf <- causal_survival_forest(
  X = data_grf$X, # covariates
  Y = data_grf$Y, # time to event
  W = data_grf$W, # treatment status
  D = data_grf$D, # event status
  horizon = 3, # 3-year horizon
  # treatment effect will be
  # measured in terms of the
  # restricted mean survival time
  target = 'RMST' 
)

fit_grf

```

## Get $\Gamma$ scores

Remember the $\Gamma_i$'s that provide conditional estimates of $\tau$? Let's get them.

```{r, echo = TRUE}

# pull the augmented CATEs from the fitted grf object
gammas <- get_scores(fit_grf)

```

- With `gammas`, we can compute ATE manually
```{r, echo=TRUE}
mean(gammas) 
```

- Verify this is what the `grf` function gives
```{r, echo=TRUE} 
average_treatment_effect(fit_grf)
```

## Your turn

Open `classwork/04-causal_forests.qmd` and complete Exercise 1

- **Reminder**: Pink sticky note for help, blue sticky when you finish.

- **Note**: the `data_coerce_grf()` function can save you lots of time.

```{r ex-1}
#| echo: false
countdown::countdown(minutes = 5, id = "ex-1")
```

## Estimating the BLP

The BLP [@semenova2021debiased]:

- Is estimated by regressing a set of covariates on $\Gamma$.

- Can be estimated for a subset of covariates 

- Can be estimated for a subset of observations.

- Summarizes heterogeneous treatment effects conditional on covariates.

You can estimate BLP manually:

```{r, echo = TRUE}

data_blp <- bind_cols(gamma = gammas, data_grf$X)

fit_blp <- lm(gamma ~ ., data = data_blp)

```

## What happens underneath the `grf` hood

Here's how you can replicate `grf` results:

:::{.column width="49%"}

```{r, echo=TRUE}
lmtest::coeftest(fit_blp, 
                 vcov = sandwich::vcovCL, 
                 type = 'HC3')
```

:::

:::{.column width="49%"}

```{r, echo=TRUE}
best_linear_projection(fit_grf, data_grf$X)
```

:::

## Your turn

Complete Exercise 2

```{r ex-2}
#| echo: false
countdown::countdown(minutes = 5, id = "ex-2")
```

## Rank-Weighted Average Treatment Effect

While ATE and BLP are helpful, they do not tell us the following:

1. How good is a *treatment prioritization rule* at distinguishing sub-populations with different conditional treatment effects?

1. Is there any heterogeneity present in a conditional treatment effect?

The rank-weighted average treatment effect (RATE) answers both of these.

## Treatment prioritization rules

Suppose we have a treatment that benefits some (not all) adults. Who should initiate treatment? A treatment prioritization rule can help:

- high score for those likely to benefit from treatment.

- low score for those likely to have a small/negative benefit from treatment.

Risk prediction models can be a treatment prioritization rule:

- Initiate antihypertensive medication if predicted risk for cardiovascular disease is high.

## How to evaluate treatment prioritization

The basic idea:

1. Chop the population up into subgroups based on the prioritization rule, e.g., by decile of score.

1. Estimate the ATE in each group, separately, and compare to the overall estimated ATE from treating everyone

1. Plot the difference between group-specific ATE and the overall ATE for each of the groups

Example: the Targeting Operator Characteristic (TOC)

## Targeting Operator Characteristic (TOC)

:::{.column width="49%"}

- Create groups by including the top q$^\text{th}$ fraction of individuals with the largest prioritization score.

- Use many values of $q$ to make the pattern more curve-like

- Motivation: Receiver Operating Characteristic (ROC) curve, a widely used metric for assessing discrimination of predictions.

:::

:::{.column width="49%"}

```{r toc-plot, echo=FALSE, fig.width=5, fig.height=5}
n <- 1000
p <- 1
X <- matrix(rnorm(n * p), n, p)
tau <- X[, 1]

ATE <- mean(tau)
sort.idx <- order(tau, decreasing = TRUE)
TOC <- rep(NA, n)
for (i in 1:n) {
  TOC[i] <- mean(tau[sort.idx[1:i]]) - ATE
}
q <- seq(1/n, 1, by = 1/n)
df <- data.frame(q, TOC)
ypoint <- df[df$q == 0.05, "TOC"]

ggplot(df, aes(x = q, y = TOC)) +
  geom_line() +
  geom_hline(yintercept = 0, lty = 3) +
  annotate(geom = 'text', x = .1, y = ypoint, color = 'black', hjust = -0.1,
           label = 'ATE of top 5% minus overall ATE') +
  annotate("segment", x = .1, xend = .05, y = ypoint, yend = ypoint,
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  theme_classic() +
  labs(y = "", title = "Targeting Operator Characteristic")
```

:::

## RATE: area underneath TOC

:::{.column width="49%"}

RATE is estimated by taking the area underneath the TOC curve.

$$\textrm{RATE} = \int_0^1 \textrm{TOC}(q) dq .$$

As $\tau(X_i)$ approaches a constant, RATE approaches 0.

:::

:::{.column width="49%"}

```{r toc-plot-autoc, echo=FALSE, fig.width=5, fig.height=5}

n <- 1000
p <- 1
X <- matrix(rnorm(n * p), n, p)
tau <- X[, 1]

ATE <- mean(tau)
sort.idx <- order(tau, decreasing = TRUE)
TOC <- rep(NA, n)
for (i in 1:n) {
  TOC[i] <- mean(tau[sort.idx[1:i]]) - ATE
}
q <- seq(1/n, 1, by = 1/n)
df <- data.frame(q, TOC)

ggplot(df, aes(x = q, y = TOC)) +
  geom_line() +
  geom_hline(yintercept = 0, lty = 3) +
  theme_classic() +
  labs(y = "", title = "Targeting Operator Characteristic") +
  geom_ribbon(aes(ymin = 0), ymax=TOC, alpha = 0.5, fill = "red") +
  annotate(geom = 'text', x = 0.15, y = 0.6, color = 'black', hjust = -0.1,
           label = "RATE")

```

:::

## RATE of prediction model

Let's use RATE to see how well risk prediction works as a treatment prioritization rule.

```{r, echo=TRUE}

library(aorsf)

fit_orsf <- orsf(time + status ~ .,
                 data = data_melodem$values,
                 oobag_pred_horizon = 3)

# important to use oobag!
prd_risk <- as.numeric(fit_orsf$pred_oobag)

prd_rate <- 
  rank_average_treatment_effect(fit_grf, 
                                priorities = prd_risk)

```

## Not good

```{r, fig.height=5, fig.width=8, out.width='100%'}
plot(prd_rate, 
     xlab = "Treated fraction", 
     main = paste("TOC of predicted risk at horizon of",
                  fit_orsf$pred_horizon))
```

## Estimating RATE from CATE

An intuitive way to assign treatment priority is to use the CATE: $\hat\tau(X_i)$

- $\hat\tau(X_i)$ should **not** be estimated and evaluated using the same data

- Use split-sample estimation or cross-fitting [@yadlowsky2021evaluating].

As a preliminary step, we'll split our data in to training and testing sets

```{r, echo=TRUE}

train_index <- sample(x = nrow(data_melodem$values), 
                      size = nrow(data_melodem$values)/2)

data_trn <- data_melodem$values[train_index, ]
data_tst <- data_melodem$values[-train_index, ]

data_trn_grf <- data_coerce_grf(data_trn)
data_tst_grf <- data_coerce_grf(data_tst)

```


---

## Fitting

We fit one forest with training data to estimate CATE and fit another forest with testing data to evaluate the CATE estimates:

:::{.column width="49%"}

```{r, echo=TRUE}
fit_trn_grf <- causal_survival_forest(
  X = data_trn_grf$X, Y = data_trn_grf$Y,
  W = data_trn_grf$W, D = data_trn_grf$D,
  horizon = 3, target = 'RMST' 
)

fit_tst_grf <- causal_survival_forest(
  X = data_tst_grf$X, Y = data_tst_grf$Y,
  W = data_tst_grf$W, D = data_tst_grf$D,
  horizon = 3, target = 'RMST' 
)
```


:::

:::{.column width="49%"}

![](diagram/rate-1.svg)
:::

## Predicting

We use the forest fitted to training data to estimate CATE for the testing data. For illustration, we also estimate naive CATE

:::{.column width="49%"}

```{r, echo=TRUE}

# the fitted forest hasn't
# seen the testing data
tau_hat_split <- fit_trn_grf %>% 
  predict(data_tst_grf$X) %>% 
  getElement("predictions")

# Illustration only (don't do this)
tau_hat_naive <- fit_trn_grf %>% 
  predict(data_trn_grf$X) %>% 
  getElement("predictions")
```

:::

:::{.column width="49%"}

![](diagram/rate-2.svg)
:::

## Evaluating

We use the forest fitted to the testing data to evaluate the CATE estimates for observations in the testing data

:::{.column width="49%"}

```{r, echo=TRUE}


rate_split <- 
  rank_average_treatment_effect(
    forest = fit_tst_grf, 
    priorities = tau_hat_split, 
    target = "AUTOC"
  )

# Illustration only (don't do this)
rate_naive <- 
  rank_average_treatment_effect(
    forest = fit_trn_grf, 
    priorities = tau_hat_naive, 
    target = "AUTOC"
  )

```

:::

:::{.column width="49%"}

![](diagram/rate-full.svg)
:::

## Correct versus overly optimistic

The problem with being overly optimistic is it has very high type 1 error

```{r, fig.width=11, fig.height=5}

data_gg <- bind_rows(
  `Correct split sample test` = rate_split$TOC,
  `Naive double dip` = rate_naive$TOC,
  .id = 'type'
)

ggplot(data_gg) + 
  aes(x = q, 
      y = estimate, 
      ymin = estimate - 1.96*std.err, 
      ymax = estimate + 1.96*std.err) +
  geom_ribbon(alpha = 0.2) + 
  geom_line() + 
  facet_wrap(~type) + 
  theme_bw() + 
  theme(panel.grid = element_blank(), 
        text = element_text(size=15)) + 
  geom_hline(yintercept = 0, linetype = 2, alpha = 0.2) + 
  labs(x = "Treated fraction (q)",
       y = "ATE at q - overall ATE")

```

## Your turn

Complete Exercise 4

```{r ex-4}
#| echo: false
countdown::countdown(minutes = 5, id = "ex-1")
```

## To the pipeline


- Copy/paste this code into your `_targets.R` file. 

```{r, eval=FALSE, echo=TRUE}

# in _targets.R, you should see this comment: 

# real data model targets (to be added as an exercise). 

# Paste this code right beneath that.

grf_shareable_zzzz_tar <- tar_target(
  grf_shareable_zzzz,
  grf_summarize(fit_grf_zzzz)
)

```

- Modify this code, replacing `zzzz` with the name of your dataset.



## References

